<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.34">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="aisic">
<meta name="dcterms.date" content="2025-08-11">
<meta name="description" content="An extended version of our ESRA talk.">

<title>Evaluating Automatic Speech Recognition (ASR) for Social Science Research: A Comparison of Semantic Metrics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../favicon.webp" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-c8ad9e5dbd60b7b70b38521ab19b7da4.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-8dfb9f1f45d5ca3c73047b52441bbe7d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script type="application/ld+json"> # JSON-LD for SEO
{
  "@context": "https://schema.org/",
  "@type": "Organization",
  "name": "AI-SIC",
  "url": "https://ai-sic.github.io",
  "alternateName": "AI Enhanced Validation of Survey Instruments",
  "description": "Research Project",
  "image": "",
  "sameAs": [
     "https://ai-sic.github.io"
  ],
}
</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="Evaluating Automatic Speech Recognition (ASR) for Social Science Research: A Comparison of Semantic Metrics">
<meta property="og:description" content="An extended version of our ESRA talk.">
<meta property="og:image" content="https://ai-sic.github.io/articles/esra_transcription_evaluation/wave.png">
<meta property="og:image:height" content="1208">
<meta property="og:image:width" content="2261">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../images/logo.png" alt="logo" class="navbar-logo">
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../articles/index.html"> 
<span class="menu-text">articles</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">about</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../contact.html"> 
<span class="menu-text">contact</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../de"> 
<span class="menu-text">de</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../.././"> 
<span class="menu-text">en</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#task-definition" id="toc-task-definition" class="nav-link" data-scroll-target="#task-definition">Task definition</a></li>
  <li><a href="#performance-of-current-asr-models" id="toc-performance-of-current-asr-models" class="nav-link" data-scroll-target="#performance-of-current-asr-models">Performance of current ASR Models</a></li>
  <li><a href="#error-metrics-for-asr-evaluation" id="toc-error-metrics-for-asr-evaluation" class="nav-link" data-scroll-target="#error-metrics-for-asr-evaluation">Error Metrics for ASR evaluation</a>
  <ul class="collapse">
  <li><a href="#semantic-metrics" id="toc-semantic-metrics" class="nav-link" data-scroll-target="#semantic-metrics">Semantic Metrics</a>
  <ul class="collapse">
  <li><a href="#weighted-wer" id="toc-weighted-wer" class="nav-link" data-scroll-target="#weighted-wer">Weighted WER</a></li>
  <li><a href="#semantically-based-error-rates" id="toc-semantically-based-error-rates" class="nav-link" data-scroll-target="#semantically-based-error-rates">Semantically based Error Rates</a></li>
  </ul></li>
  <li><a href="#our-proposal-visually-aided-semantic-evaluation" id="toc-our-proposal-visually-aided-semantic-evaluation" class="nav-link" data-scroll-target="#our-proposal-visually-aided-semantic-evaluation">Our proposal: Visually aided Semantic Evaluation</a></li>
  </ul></li>
  <li><a href="#lessons-learned-and-future-directions" id="toc-lessons-learned-and-future-directions" class="nav-link" data-scroll-target="#lessons-learned-and-future-directions">Lessons Learned and future Directions</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Evaluating Automatic Speech Recognition (ASR) for Social Science Research: A Comparison of Semantic Metrics</h1>
</div>

<div>
  <div class="description">
    An extended version of our ESRA talk.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Peter Kannewitz</div>
    <div class="quarto-title-meta-contents">
             <p><a href="https://ai-sic.github.io/">aisic</a> </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">August 11, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<ul>
<li><p>conference programm: <a href="https://www.europeansurveyresearch.org/conf2025/prog.php?sess=99#326">ESRA Conference</a>.</p></li>
<li><p>presentation slides: <a href="../../static/files/esra_asr.pdf">Slides</a>.</p></li>
</ul>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>This July, our team attended the ESRA conference in Utrecht (NL) and gave two talks about our intermediate findings. In the following, I will outline the main arguments on what needs to be considered when using transcription models for social science research. If you are interested in synthetic data generation with LLMs—a topic that was also widely discussed at the conference—you should take a look at our talk on <a href="../../articles/esra_silicon_interviews/">Silicon Interviews</a>.</p>
</section>
<section id="task-definition" class="level1">
<h1>Task definition</h1>
<p>In <a href="../../about.html">our research project</a>, we conduct interviews with German-speaking children aged 8 to 16 and their parents. The children are first asked to assess their health on a scale and then to explain their reasoning. Our sociological research interest is to illuminate children’s self and proxy-rating strategies. At the same time, as part of the <a href="https://www.new-data-spaces.de">New Data Spaces</a> program of the DFG, we pursue a second goal: finding new ways to semi-automate the traditional research process by employing state-of-the-art technologies.</p>
<p>This brings us to the main topic of this write-up: given the recordings of our interviews, can we simply use a recent ASR model to transcribe them, or do these models produce errors that are unacceptable for our downstream task? And how can we even evaluate this? We are convinced that this question is relevant not only for our research, but also for a broad range of studies that deal with audio data of any kind.</p>
<div class="figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/task_overview.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Our ASR Pipeline. Own illustration.</figcaption>
</figure>
</div>
</div>
<p>Before exploring the possibilities for answering this question, I will briefly introduce the most important terminology. <strong>ASR</strong> (Automatic Speech Recognition) refers to the broader task of extracting human utterances from a speech signal. This concept includes various interrelated subtasks, such as Speech Recognition, Speaker Diarization, Speaker Detection, Language Detection, and Transcription. In the following, we will mainly focus on the transcript as the final result of the pipeline. This is partly because current transformer-based neural networks such as <a href="https://openai.com/index/whisper/">Whisper</a> combine many of these tasks in a single model, allowing us to focus on the outcome.</p>
<p>We call the transcript produced by the model the <em>hypothesis</em>, which we aim to align with a <em>reference</em>—a transcribed version we consider complete.</p>
</section>
<section id="performance-of-current-asr-models" class="level1">
<h1>Performance of current ASR Models</h1>
<p>One might ask: Isn’t ASR a solved task? A glance at the following figure might suggest exactly that:</p>
<div class="figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/librispeech_wer.png" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>Word Error Rate (WER) for LibreSpeech (Read English Speech). <em>Source: <a href="https://awni.github.io/future-speech/" class="uri">https://awni.github.io/future-speech/</a></em></figcaption>
</figure>
</div>
</div>
<p>It shows declining error rates over time. Notably, the dashed horizontal lines represent human reference scores for this dataset. On <a href="https://www.openslr.org/12/">LibreSpeech</a> (read English speech), ASR models have matched or even out-performed humans for years.</p>
<p>However, there’s more to the story when we change the context. Consider a table from the Whisper paper by <span class="citation" data-cites="radford_robust_2022">Radford et al. (<a href="#ref-radford_robust_2022" role="doc-biblioref">2022</a>)</span>, which reports WER on the VoxPopuli corpus—European Parliament recordings:</p>
<!-- Table -->
<table class="caption-top table">
<caption>WER (%) on VoxPopuli corpus for selected languages. <em>Source: <span class="citation" data-cites="radford_robust_2022">Radford et al. (<a href="#ref-radford_robust_2022" role="doc-biblioref">2022</a>)</span>, p.&nbsp;23</em></caption>
<thead>
<tr class="header">
<th>Model</th>
<th>Czech</th>
<th>German</th>
<th>English</th>
<th>en_accented</th>
<th>…</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Whisper tiny</td>
<td>73.5</td>
<td>27.4</td>
<td>11.6</td>
<td>18.8</td>
<td>…</td>
</tr>
<tr class="even">
<td>Whisper base</td>
<td>54.7</td>
<td>20.6</td>
<td>9.5</td>
<td>17.5</td>
<td>…</td>
</tr>
<tr class="odd">
<td>Whisper small</td>
<td>28.8</td>
<td>14.8</td>
<td>8.2</td>
<td>19.2</td>
<td>…</td>
</tr>
<tr class="even">
<td>Whisper medium</td>
<td>18.4</td>
<td>12.4</td>
<td>7.6</td>
<td>19.1</td>
<td>…</td>
</tr>
<tr class="odd">
<td>Whisper large</td>
<td>15.9</td>
<td>11.9</td>
<td>7.2</td>
<td>20.8</td>
<td>…</td>
</tr>
<tr class="even">
<td>Whisper large-v2</td>
<td>12.6</td>
<td><strong>11.2</strong></td>
<td><strong>7.0</strong></td>
<td><strong>18.6</strong></td>
<td>…</td>
</tr>
</tbody>
</table>
<!-- Table -->
<p>Even though the same error rate metric is used as in the previous plot, the overall error here is much higher. This difference simply comes from a change in setting. Instead of read English speech, the <a href="https://github.com/facebookresearch/voxpopuli">VoxPopuli</a> dataset covers European Parliament event recordings. In addition to the higher baseline, the individual error rates also vary dramatically. If there is an accent in the English speech, the average error rate rises to 18.6 for the best-performing model. This translates to almost every fifth word being incorrectly transcribed. For our target language, German, we can also see that the errors are generally higher.</p>
<p>So what now? How can we know if ASR errors are acceptable for our downstream task? To find an answer to this question, we first need to take a look at different error metrics.</p>
</section>
<section id="error-metrics-for-asr-evaluation" class="level1">
<h1>Error Metrics for ASR evaluation</h1>
<p>As we have done so far, one way to judge the quality of ASR models is by using error metrics. The most common reference metric for this task is the Word Error Rate (<strong>WER</strong>). It is computed based on an aligned version of the reference and the hypothesis as follows:</p>
<p><span class="math display">
WER = \frac{\text{Substitutions} + \text{Deletions} + \text{Insertions}}{\text{Total Number of Words}}
</span></p>
<p>In the following table, we can see two examples of substitution errors. In both cases, a word from the reference (ground truth) was incorrectly transcribed by the ASR model:</p>
<!-- Table -->
<table class="caption-top table">
<caption>WER calculated by equal weighting of word errors.</caption>
<colgroup>
<col style="width: 17%">
<col style="width: 63%">
<col style="width: 19%">
</colgroup>
<thead>
<tr class="header">
<th><strong>System</strong></th>
<th><strong>Transcription</strong></th>
<th><strong>WER (%)</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Reference</td>
<td>Find me flights to London</td>
<td>0.0</td>
</tr>
<tr class="even">
<td>ASR 1</td>
<td>Find <span style="color:red">the</span> flights to London</td>
<td><span style="color:red">20.0</span></td>
</tr>
<tr class="odd">
<td>ASR 2</td>
<td>Find me flights to <span style="color:red">Lisbon</span></td>
<td><span style="color:red">20.0</span></td>
</tr>
</tbody>
</table>
<!-- Table -->
<p>The first model substituted “me” with “the,” which is one error out of five words in total, yielding an error rate of 20%. The second hypothesis gives the same error rate by incorrectly transcribing “Lisbon” instead of “London.”</p>
<p>This simple example shows some shortcomings of WER. Most importantly, the equal weighting of all words does not allow us to judge the <em>completeness</em> of the transcript with respect to the task at hand. For example, if we were researching personal pronouns, ASR 1 would yield a task-specific error rate of 100%. If we were trying to identify flight destinations, ASR 2 would yield a task-specific error rate of 100%.</p>
<p>To tackle this problem, alternative metrics have been proposed in the literature. Next, we will take a brief look at the ideas behind them and how they can help us achieve our goal: assessing whether ASR models are useful for automatically transcribing our interviews.</p>
<section id="semantic-metrics" class="level2">
<h2 class="anchored" data-anchor-id="semantic-metrics">Semantic Metrics</h2>
<p>In general, we can identify two ways to obtain more meaningful metrics. The first way is to improve the word error rate by weighting different classes of errors differently. For example, we could assign higher error penalties when nouns are incorrectly transcribed and lower penalties when articles are incorrect.</p>
<p>Instead of this approach, there are also metrics that move away from measuring lexical errors altogether. These use so-called word embeddings to compare whether the reference and hypothesis align semantically.</p>
<p>We will discuss one representative from each group to get a better sense of what they can tell us about transcription quality. It is worth noting that we set up these meta-categories to better understand what the metrics do, but the different representatives within these categories still share common principles, such as alignment and embedding strategies.</p>
<section id="weighted-wer" class="level3">
<h3 class="anchored" data-anchor-id="weighted-wer">Weighted WER</h3>
<p>A good example of the weighted WER is the Semantic-WER by <span class="citation" data-cites="roy_semantic-wer_2021">Roy (<a href="#ref-roy_semantic-wer_2021" role="doc-biblioref">2021</a>)</span>. It uses a sophisticated ruleset for each of the WER error types (Substitution, Deletion, Insertion). This formula is taken from <span class="citation" data-cites="roy_semantic-wer_2021">Roy (<a href="#ref-roy_semantic-wer_2021" role="doc-biblioref">2021</a>)</span> page 2:</p>
<div class="figure">
<p><span class="math display">
W_{\text{sub}} =
\begin{cases}
{1,} &amp; {\text{if } r_w \in \text{NE} \cup \text{SENT}} \\
\text{cer}(r_w, h_w), &amp; \text{if } r_w \in \text{SE} \\
\dots &amp;\\
\end{cases}
</span></p>
</div>
<p>Just looking at one of these rulesets for substitutions, we can see that different kinds of words are assigned different error weights. If the reference word (<span class="math inline">r_w</span>) substituted by another word was a named entity (NE) or a sentiment word (SENT), an error of 1 is assigned. This rule, for example, ensures that substitution of place names such as “London” contributes to a higher overall error.</p>
<p>The second case accounts for spelled-out entities (SE), for example in telephone interviews, where we compute how many characters are incorrect using the character error rate between reference and hypothesis (<span class="math inline">cer(r_w, h_w)</span>).</p>
<p>The list goes on, but from here the idea should be clear. Another simpler way to weigh errors is used by the Embedding Error Rater (EmbER) by <span class="citation" data-cites="roux_qualitative_2022">Roux et al. (<a href="#ref-roux_qualitative_2022" role="doc-biblioref">2022</a>)</span>, which weighs errors by distance in embedding space.</p>
</section>
<section id="semantically-based-error-rates" class="level3">
<h3 class="anchored" data-anchor-id="semantically-based-error-rates">Semantically based Error Rates</h3>
<p>Moving away from comparison based on alignment at the lexical level, the <strong>BERTScore</strong> by <span class="citation" data-cites="zhang_bertscore_2020">Zhang et al. (<a href="#ref-zhang_bertscore_2020" role="doc-biblioref">2020</a>)</span> first embeds every single token of the reference and hypothesis using a BERT encoder. This approach leverages the idea that in the embedding space, similar words tend to be closer to each other. Similarity is assumed to be reflected across multiple dimensions—not only lexical or grammatical ones.</p>
<p>For example, in the following projection of an embedding space, the relationship between countries and their capitals is reflected, along with some notion of geographical proximity.</p>
<div class="figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/word2vec_country_capital.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Word Embeddings Concept. <em>Source: <a href="https://lovit.github.io/assets/figures/word2vec_country_capital.png" class="uri">https://lovit.github.io/assets/figures/word2vec_country_capital.png</a>.</em></figcaption>
</figure>
</div>
</div>
<p>After obtaining the embedding vectors, BERTScore aligns reference and hypothesis tokens based on proximity in embedding space. Then, token-based cosine similarity is used to compute a similarity score between aligned tokens. This similarity is weighted by TF-IDF (Term Frequency-Inverse Document Frequency) to give more importance to terms that are more characteristic of the document.</p>
<p>This idea is also employed by several other metrics that differ slightly in implementation. The <strong>Semantic Distance (SemDist)</strong> by <span class="citation" data-cites="kim_semantic_2021">Kim et al. (<a href="#ref-kim_semantic_2021" role="doc-biblioref">2021</a>)</span> uses sentence-level embeddings. The <strong>Aligned Semantic Distance (ASD)</strong> by <span class="citation" data-cites="rugayan_perceptual_2023">Rugayan, Salvi, and Svendsen (<a href="#ref-rugayan_perceptual_2023" role="doc-biblioref">2023</a>)</span> applies character-level alignment and weighs edit-level errors by semantic proximity of segments. The <strong>SeMaScore</strong> by <span class="citation" data-cites="sasindran_semascore_2024">Sasindran, Yelchuri, and Prabhakar (<a href="#ref-sasindran_semascore_2024" role="doc-biblioref">2024</a>)</span> uses dynamic programming for alignment, proceeding in a similar way as ASD. These are just a few examples highlighting key differences in semantic error metrics.</p>
</section>
</section>
<section id="our-proposal-visually-aided-semantic-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="our-proposal-visually-aided-semantic-evaluation">Our proposal: Visually aided Semantic Evaluation</h2>
<p>After reviewing various error metrics, the key question remains: can they truly help us evaluate transcription models for our specific needs? While it’s valuable to assess transcripts beyond just lexical accuracy—and combining different metrics can reveal the types of errors—looking at scores aggregated over an entire transcript only gives a general, averaged picture. It doesn’t show us <em>where</em> or <em>how</em> the errors happen throughout the interview.</p>
<p>To overcome this limitation and give researchers deeper insight into transcript quality, we developed a notebook that tracks errors over time—that is, across the length of the transcript.</p>
<div class="figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/window_eval_2_larger.png" class="img-fluid figure-img"></p>
<figcaption>Our proposal: Interactive analysis of errors (Screenshot). Full code under: <a href="https://github.com/ai-sic/semantic-asr-evaluation" class="uri">https://github.com/ai-sic/semantic-asr-evaluation</a>.</figcaption>
</figure>
</div>
</div>
<p>Specifically, we chunk a transcript into segments and measure the error for each segment based on similarity in embedding space. This approach enables interactive investigation of how errors vary throughout the transcript. It also helps identify problematic parts of the data.</p>
<p>In the example figure, the first part of the transcript is missing, which could be caused by poor audio quality or background noise during that segment.</p>
</section>
</section>
<section id="lessons-learned-and-future-directions" class="level1">
<h1>Lessons Learned and future Directions</h1>
<p>This article began by introducing the ASR task and reviewing the performance of current models. We then highlighted that the question of “how good” an ASR system is always depends on the specific goals you want to achieve—there is no one-size-fits-all criterion, especially considering the inevitable trade-offs between different types of errors.</p>
<p>To better assess how suitable ASR models are for your data and research questions, we proposed several metrics that go beyond the classic WER. Additionally, we provided code to move from global summary scores toward interactive, segment-level analysis of errors.</p>
<p>Based on our work, we propose the following guidelines for evaluating ASR in social science research:</p>
<ul>
<li><strong>Evaluate according to your task:</strong> Before starting, identify which aspects of the transcript are critical to answering your research questions.</li>
<li><strong>Combine different error metrics:</strong> Use multiple metrics to get a fuller picture of transcript quality.</li>
<li><strong>Interactively analyze error classes:</strong> Explore where and what kinds of errors occur within transcripts to understand their impact.</li>
</ul>
<p>From applying these evaluation steps to our pre-test data, we learned several important lessons for the upcoming field phase:</p>
<ul>
<li><strong>Audio quality is crucial:</strong> Aim for the best possible recording conditions, as some audio artifacts may not be noticeable to human listeners but can significantly degrade ASR performance.</li>
<li><strong>Overlapping speech degrades transcripts:</strong> In interview settings, overlapping talk strongly reduces transcript accuracy.</li>
<li><strong>Creating a reliable ground truth is challenging:</strong> Preparing human-annotated transcripts is notoriously difficult and some observed errors may actually stem from annotator oversights.</li>
<li><strong>Model adaptation can help:</strong> Fine-tuning or prompting ASR models for specific speaker groups may improve results. Transformer-based models like Whisper tend to produce overly polished transcripts, smoothing out dialects, interjections, and slips of the tongue, which might be important for your analysis.</li>
</ul>
<p>A more detailed report on our transcription experiements will be published soon.</p>
<!-- # Abstract -->
<!-- Automatic Speech Recognition (ASR) is an essential technology for automating the transcription of qualitative data in social science research, particularly with large interview datasets. Recent advancements in ASR have introduced powerful new tools to the field, but their implementation requires careful and thoughtful consideration to ensure reliability and accuracy. Since outcomes vary significantly depending on the model and its (hyper-)parametrization, it is crucial to evaluate the generalization capabilities of ASR models on specific research data using a meaningful and comparable metric. Addressing these challenges will enable social scientists to effectively leverage these technologies in their research. -->
<!-- The most commonly used metric for this purpose is the Word Error Rate (WER). WER depends on specific language-specific text transformations and focuses on surface-level accuracy, making it inadequate for evaluating transcript quality in social sciences and downstream NLP tasks. To address limitations, modern, semantics-oriented metrics have been developed in recent years. Metrics such as Embedding Error Rate (EmbER) and Semantic-WER apply penalties for different types of errors, while methods like BERTScore, SeMaScore, SemDist, and Aligned Semantic Distance (ASD) improve evaluation by utilizing contextual embeddings and advanced matching techniques to assess semantic similarity. -->
<!-- Our research centers on comparing the usability of these semantic metrics for ASR in social sciences and developing a more intuitive approach for analyzing semantic differences in ASR transcriptions using aligned window-based semantic comparison, as opposed to relying on traditional singular value metrics. The proposed talk is not only designed to improve the quality of individual research projects but also to contribute to the creation of new data spaces for the social sciences, where ASR is a fundamental technology. Only when developing robust methods for evaluating and utilizing ASR, we can unlock the potential of large-scale qualitative datasets, opening up new avenues for research and analysis. -->



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-kim_semantic_2021" class="csl-entry" role="listitem">
Kim, Suyoun, Abhinav Arora, Duc Le, Ching-Feng Yeh, Christian Fuegen, Ozlem Kalinli, and Michael L. Seltzer. 2021. <span>“Semantic <span>Distance</span>: <span>A</span> <span>New</span> <span>Metric</span> for <span>ASR</span> <span>Performance</span> <span>Analysis</span> <span>Towards</span> <span>Spoken</span> <span>Language</span> <span>Understanding</span>.”</span> In <em>Interspeech 2021</em>, 1977–81. ISCA. <a href="https://doi.org/10.21437/Interspeech.2021-1929">https://doi.org/10.21437/Interspeech.2021-1929</a>.
</div>
<div id="ref-radford_robust_2022" class="csl-entry" role="listitem">
Radford, Alec, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2022. <span>“Robust <span>Speech</span> <span>Recognition</span> via <span>Large</span>-<span>Scale</span> <span>Weak</span> <span>Supervision</span>.”</span> arXiv. <a href="http://arxiv.org/abs/2212.04356">http://arxiv.org/abs/2212.04356</a>.
</div>
<div id="ref-roux_qualitative_2022" class="csl-entry" role="listitem">
Roux, Thibault Bañeras, Mickael Rouvier, Jane Wottawa, and Richard Dufour. 2022. <span>“Qualitative <span>Evaluation</span> of <span>Language</span> <span>Model</span> <span>Rescoring</span> in <span>Automatic</span> <span>Speech</span> <span>Recognition</span>.”</span> In <em>Interspeech 2022</em>, 3968–72. ISCA. <a href="https://doi.org/10.21437/Interspeech.2022-10931">https://doi.org/10.21437/Interspeech.2022-10931</a>.
</div>
<div id="ref-roy_semantic-wer_2021" class="csl-entry" role="listitem">
Roy, Somnath. 2021. <span>“Semantic-<span>WER</span>: <span>A</span> <span>Unified</span> <span>Metric</span> for the <span>Evaluation</span> of <span>ASR</span> <span>Transcript</span> for <span>End</span> <span>Usability</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2106.02016">https://doi.org/10.48550/arXiv.2106.02016</a>.
</div>
<div id="ref-rugayan_perceptual_2023" class="csl-entry" role="listitem">
Rugayan, Janine, Giampiero Salvi, and Torbjørn Svendsen. 2023. <span>“Perceptual and <span>Task</span>-<span>Oriented</span> <span>Assessment</span> of a <span>Semantic</span> <span>Metric</span> for <span>ASR</span> <span>EvaluatiPerceptual</span> and <span>Task</span>-<span>Oriented</span> <span>Assessment</span> of a <span>Semantic</span> <span>Metric</span> for <span>ASR</span> <span>Evaluationon</span>.”</span> In <em><span>INTERSPEECH</span> 2023</em>, 2158–62. ISCA. <a href="https://doi.org/10.21437/Interspeech.2023-1778">https://doi.org/10.21437/Interspeech.2023-1778</a>.
</div>
<div id="ref-sasindran_semascore_2024" class="csl-entry" role="listitem">
Sasindran, Zitha, Harsha Yelchuri, and T. V. Prabhakar. 2024. <span>“<span>SeMaScore</span>: <span>A</span> New Evaluation Metric for Automatic Speech Recognition Tasks.”</span> In <em>Interspeech 2024</em>, 4558–62. ISCA. <a href="https://doi.org/10.21437/Interspeech.2024-2033">https://doi.org/10.21437/Interspeech.2024-2033</a>.
</div>
<div id="ref-zhang_bertscore_2020" class="csl-entry" role="listitem">
Zhang, Tianyi, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. <span>“<span>BERTScore</span>: <span>Evaluating</span> <span>Text</span> <span>Generation</span> with <span>BERT</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1904.09675">https://doi.org/10.48550/arXiv.1904.09675</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/ai-sic\.github\.io");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 © 2025 AI-SIC
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/AI-SIC">
      <i class="bi bi-github" role="img" aria-label="GitHub">
</i> 
    </a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../imprint.html">
<p>Imprint</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../privacy-statement.html">
<p>Privacy Statement</p>
</a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Built with Quarto, inspired by martenw.com
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>