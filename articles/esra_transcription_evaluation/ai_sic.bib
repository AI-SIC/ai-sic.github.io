@inproceedings{rugayan_perceptual_2023,
	title = {Perceptual and {Task}-{Oriented} {Assessment} of a {Semantic} {Metric} for {ASR} {EvaluatiPerceptual} and {Task}-{Oriented} {Assessment} of a {Semantic} {Metric} for {ASR} {Evaluationon}},
	url = {https://www.isca-archive.org/interspeech_2023/rugayan23_interspeech.html},
	doi = {10.21437/Interspeech.2023-1778},
	language = {en},
	urldate = {2025-05-07},
	booktitle = {{INTERSPEECH} 2023},
	publisher = {ISCA},
	author = {Rugayan, Janine and Salvi, Giampiero and Svendsen, Torbjørn},
	month = aug,
	year = {2023},
	pages = {2158--2162},
}

@inproceedings{kim_semantic_2021,
	title = {Semantic {Distance}: {A} {New} {Metric} for {ASR} {Performance} {Analysis} {Towards} {Spoken} {Language} {Understanding}},
	shorttitle = {Semantic {Distance}},
	url = {https://www.isca-archive.org/interspeech_2021/kim21e_interspeech.html},
	doi = {10.21437/Interspeech.2021-1929},
	language = {en},
	urldate = {2025-05-07},
	booktitle = {Interspeech 2021},
	publisher = {ISCA},
	author = {Kim, Suyoun and Arora, Abhinav and Le, Duc and Yeh, Ching-Feng and Fuegen, Christian and Kalinli, Ozlem and Seltzer, Michael L.},
	month = aug,
	year = {2021},
	pages = {1977--1981},
}

@misc{roy_semantic-wer_2021,
	title = {Semantic-{WER}: {A} {Unified} {Metric} for the {Evaluation} of {ASR} {Transcript} for {End} {Usability}},
	shorttitle = {Semantic-{WER}},
	url = {http://arxiv.org/abs/2106.02016},
	doi = {10.48550/arXiv.2106.02016},
	abstract = {Recent advances in supervised, semi-supervised and self-supervised deep learning algorithms have shown significant improvement in the performance of automatic speech recognition(ASR) systems. The state-of-the-art systems have achieved a word error rate (WER) less than 5\%. However, in the past, researchers have argued the non-suitability of the WER metric for the evaluation of ASR systems for downstream tasks such as spoken language understanding (SLU) and information retrieval. The reason is that the WER works at the surface level and does not include any syntactic and semantic knowledge.The current work proposes Semantic-WER (SWER), a metric to evaluate the ASR transcripts for downstream applications in general. The SWER can be easily customized for any down-stream task.},
	urldate = {2025-05-07},
	publisher = {arXiv},
	author = {Roy, Somnath},
	month = oct,
	year = {2021},
	note = {arXiv:2106.02016},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{zhang_bertscore_2020,
	title = {{BERTScore}: {Evaluating} {Text} {Generation} with {BERT}},
	shorttitle = {{BERTScore}},
	url = {http://arxiv.org/abs/1904.09675},
	doi = {10.48550/arXiv.1904.09675},
	abstract = {We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that BERTScore is more robust to challenging examples when compared to existing metrics.},
	urldate = {2025-05-07},
	publisher = {arXiv},
	author = {Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q. and Artzi, Yoav},
	month = feb,
	year = {2020},
	note = {arXiv:1904.09675},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{sasindran_semascore_2024,
	title = {{SeMaScore}: {A} new evaluation metric for automatic speech recognition tasks},
	shorttitle = {{SeMaScore}},
	url = {https://www.isca-archive.org/interspeech_2024/sasindran24_interspeech.html},
	doi = {10.21437/Interspeech.2024-2033},
	language = {en},
	urldate = {2025-05-07},
	booktitle = {Interspeech 2024},
	publisher = {ISCA},
	author = {Sasindran, Zitha and Yelchuri, Harsha and Prabhakar, T. V.},
	month = sep,
	year = {2024},
	pages = {4558--4562},
}

@inproceedings{rumberg_improving_2022,
	title = {Improving {Phonetic} {Transcriptions} of {Children}’s {Speech} by {Pronunciation} {Modelling} with {Constrained} {CTC}-{Decoding}},
	url = {https://www.isca-archive.org/interspeech_2022/rumberg22b_interspeech.html},
	doi = {10.21437/Interspeech.2022-332},
	language = {en},
	urldate = {2025-05-06},
	booktitle = {Interspeech 2022},
	publisher = {ISCA},
	author = {Rumberg, Lars and Gebauer, Christopher and Ehlert, Hanna and Lüdtke, Ulrike and Ostermann, Jörn},
	month = sep,
	year = {2022},
	pages = {1357--1361},
}

@article{stuckey_first_2014,
	title = {The first step in {Data} {Analysis}: {Transcribing} and managing qualitative research data},
	volume = {02},
	copyright = {Thieme Medical and Scientific Publishers Private Ltd. A-12, Second Floor, Sector -2, NOIDA -201301, India},
	issn = {2321-0656, 2321-0664},
	shorttitle = {The first step in {Data} {Analysis}},
	url = {http://www.thieme-connect.de/DOI/DOI?10.4103/2321-0656.120254},
	doi = {10.4103/2321-0656.120254},
	abstract = {{\textless}p{\textgreater}Researchers need to take data from the spoken text (structured, unstructured, or narrative interviews) to written form for analysis. Typically this is handled through deidentifying the participants and transcribing the data, and is considered the first step in analysis. The accuracy of the transcription plays a role in determining the accuracy of the data that are analyzed and with what degree of dependability. Analysis begins after reviewing the first interview to examine whether participants are responding to the research question related to your area of interest in diabetes, or whether your interview guide needs refining. As each interview is completed, the researcher examines its content to determine what has been learned and what still needs to be discovered or needs elaboration. Moving from raw interviews to evidence-based interpretations requires preparing transcripts so they will be ready to code. Before moving directing to analysis (or coding), it is important to recognize the task of handling the qualitative research data during and after the interview. This paper describes the process of transcription and handling the qualitative data related to diabetes research.{\textless}/p{\textgreater}},
	language = {en},
	number = {01},
	urldate = {2025-04-11},
	journal = {Journal of Social Health and Diabetes},
	author = {Stuckey, Heather L.},
	month = jun,
	year = {2014},
	note = {Publisher: Thieme Medical and Scientific Publishers Private Ltd.},
	pages = {006--008},
	file = {Full Text PDF:/home/pet/Zotero/storage/EHN7X466/Stuckey - 2014 - The first step in Data Analysis Transcribing and managing qualitative research data.pdf:application/pdf},
}

@article{oliver_constraints_2005,
	title = {Constraints and {Opportunities} with {Interview} {Transcription}: {Towards} {Reflection} in {Qualitative} {Research}},
	volume = {84},
	issn = {0037-7732},
	shorttitle = {Constraints and {Opportunities} with {Interview} {Transcription}},
	url = {https://doi.org/10.1353/sof.2006.0023},
	doi = {10.1353/sof.2006.0023},
	abstract = {In this paper we discuss the complexities of interview transcription. While often seen as a behind-the-scenes task, we suggest that transcription is a powerful act of representation. Transcription is practiced in multiple ways, often using naturalism, in which every utterance is captured in as much detail as possible, and/or denaturalism, in which grammar is corrected, interview noise (e.g., stutters, pauses, etc.) is removed and non- standard accents (i.e., non-majority) are standardized. In this article, we discuss the constraints and opportunities of our transcription decisions and point to an intermediate, reflective step. We suggest that researchers incorporate reflection into their research design by interrogating their transcription decisions and the possible impact these decisions may have on participants and research outcomes.},
	number = {2},
	urldate = {2025-04-11},
	journal = {Social Forces},
	author = {Oliver, Daniel G. and Serovich, Julianne M. and Mason, Tina L.},
	month = dec,
	year = {2005},
	pages = {1273--1289},
	file = {Full Text PDF:/home/pet/Zotero/storage/ZGAMCWAE/Oliver et al. - 2005 - Constraints and Opportunities with Interview Transcription Towards Reflection in Qualitative Resear.pdf:application/pdf;Snapshot:/home/pet/Zotero/storage/ALG2S2UB/2234980.html:text/html},
}

@article{wellard_turning_2001,
	title = {Turning tapes into text: issues surrounding the transcription of interviews},
	volume = {11},
	issn = {1037-6178},
	shorttitle = {Turning tapes into text},
	url = {https://doi.org/10.5172/conu.11.2-3.180},
	doi = {10.5172/conu.11.2-3.180},
	abstract = {Transcription of interview data is a common practice in qualitative health research. However, there has been little discussion of the techniques of transcription and the issues inherent in the use of transcription as a strategy for managing qualitative data in nursing publications. The process of transcription may disclose or obscure certain information. Researchers need to question practices of transcription that have been taken for granted and make transparent the processes used to preserve the integrity of data. This paper first examines research reported in nursing and allied health journals employing interviews for data collection and the attention given to the transcription phase. It then deals with issues of concern regarding the transcription of interviews, and offers suggestions for promoting validity.},
	number = {2-3},
	urldate = {2025-04-11},
	journal = {Contemporary Nurse},
	author = {Wellard, Sally and and McKenna, Lisa},
	month = dec,
	year = {2001},
	pmid = {11924614},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.5172/conu.11.2-3.180},
	keywords = {data management, qualitative interview, transcription},
	pages = {180--186},
}

@article{sanchez-guardiola_paredes_content_2021,
	title = {Content {Validation} of a {Semi}-{Structured} {Interview} to {Analyze} the {Management} of {Suffering}},
	volume = {18},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1660-4601},
	url = {https://www.mdpi.com/1660-4601/18/21/11393},
	doi = {10.3390/ijerph182111393},
	abstract = {This work involves the content validation of a semi-structured interview, whose objective is to learn about the management of suffering in people. The interview items have been classified into several categories that define the suffering construct. For the content validation of the instrument, in addition to initially conducting a scientific review on the subject, the procedure known as expert judgement has been used. The results obtained in terms of the content validity achieved in the dimensions and areas assessed are, in general, satisfactory. However, some of these dimensions and certain areas have not exceeded the required minimum values for content validity. Therefore, it is necessary to modify the items comprising these dimensions in the areas evaluated with the additional incorporation of the qualitative suggestions for improvement indicated by the experts. As for agreement among experts, the results point to moderate agreement, which, moreover, is not due to chance.},
	language = {en},
	number = {21},
	urldate = {2025-04-02},
	journal = {International Journal of Environmental Research and Public Health},
	author = {Sánchez-Guardiola Paredes, Carmen and Aguaded Ramírez, Eva María and Rodríguez-Sabiote, Clemente},
	month = jan,
	year = {2021},
	note = {Number: 21
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {concordance, content validity, expert judgement, suffering},
	pages = {11393},
	file = {Full Text PDF:/home/pet/Zotero/storage/I5LEEY65/Sánchez-Guardiola Paredes et al. - 2021 - Content Validation of a Semi-Structured Interview to Analyze the Management of Suffering.pdf:application/pdf},
}

@inproceedings{parfenova_automating_2024,
	address = {New York, NY, USA},
	series = {{WWW} '24},
	title = {Automating the {Information} {Extraction} from {Semi}-{Structured} {Interview} {Transcripts}},
	isbn = {979-8-4007-0172-6},
	url = {https://dl.acm.org/doi/10.1145/3589335.3651230},
	doi = {10.1145/3589335.3651230},
	abstract = {This paper explores the development and application of an automated system designed to extract information from semi-structured interview transcripts. Given the labor-intensive nature of traditional qualitative analysis methods, such as coding, there exists a significant demand for tools that can facilitate the analysis process. Our research investigates various topic modeling techniques and concludes that the best model for analyzing interview texts is a combination of BERT embeddings and HDBSCAN clustering. We present a user-friendly software prototype that enables researchers, including those without programming skills, to efficiently process and visualize the thematic structure of interview data. This tool not only facilitates the initial stages of qualitative analysis but also offers insights into the interconnectedness of topics revealed, thereby enhancing the depth of qualitative analysis.},
	urldate = {2025-02-13},
	booktitle = {Companion {Proceedings} of the {ACM} {Web} {Conference} 2024},
	publisher = {Association for Computing Machinery},
	author = {Parfenova, Angelina},
	year = {2024},
	pages = {983--986},
	annote = {Gemeinsames Ziel: 
Qualitative Forschung automatisieren, insbesondere die Kodierung und die Themenextraktion

ML-Einbindung in Qualidatenauswertung
BERT und HDBSCAN
Erweiterung von uns: Active Learning für mehr Kontrolle und Verbesserung durch menschliches Feedback und mehr Anpassungsfähigkeit
},
	file = {Full Text PDF:/home/pet/Zotero/storage/4KFQNB7T/Parfenova - 2024 - Automating the Information Extraction from Semi-Structured Interview Transcripts.pdf:application/pdf},
}

@misc{noauthor_computational_nodate,
	title = {Computational {Coding} in {Interview} {Analysis} {\textbar} {Elicit}},
	url = {https://elicit.com/notebook/634734dd-abc5-4df9-b3fc-8255a01e41ee#1823b9266c1f52677494107d32587710},
	urldate = {2025-02-13},
	file = {Computational Coding in Interview Analysis | Elicit:/home/pet/Zotero/storage/WVTZ2MC9/634734dd-abc5-4df9-b3fc-8255a01e41ee.html:text/html},
}

@incollection{kuckartz_kategorien_2010,
	address = {Wiesbaden},
	title = {Die {Kategorien} und das {Codieren} von {Texten}},
	isbn = {978-3-531-16661-2 978-3-531-92126-6},
	url = {http://link.springer.com/10.1007/978-3-531-92126-6_3},
	language = {de},
	urldate = {2025-02-13},
	booktitle = {Einführung in die computergestützte {Analyse} qualitativer {Daten}},
	publisher = {VS Verlag für Sozialwissenschaften},
	author = {Kuckartz, Udo},
	collaborator = {Kuckartz, Udo},
	year = {2010},
	doi = {10.1007/978-3-531-92126-6_3},
	pages = {57--71},
	file = {Full Text PDF:/home/pet/Zotero/storage/Z8Z5HLG3/Kuckartz - 2010 - Die Kategorien und das Codieren von Texten.pdf:application/pdf},
}

@article{garbarski_how_2017,
	title = {How participants report their health status: cognitive interviews of self-rated health across race/ethnicity, gender, age, and educational attainment},
	volume = {17},
	issn = {1471-2458},
	shorttitle = {How participants report their health status},
	url = {https://doi.org/10.1186/s12889-017-4761-2},
	doi = {10.1186/s12889-017-4761-2},
	abstract = {Self-rated health (SRH) is widely used to measure subjective health. Yet it is unclear what underlies health ratings, with implications for understanding the validity of SRH overall and across sociodemographic characteristics. We analyze participants’ explanations of how they formulated their SRH answer in addition to which health factors they considered and examine group differences in these processes.},
	number = {1},
	urldate = {2025-02-10},
	journal = {BMC Public Health},
	author = {Garbarski, Dana and Dykema, Jennifer and Croes, Kenneth D. and Edwards, Dorothy F.},
	month = oct,
	year = {2017},
	keywords = {Cognitive interviewing, Evaluative frameworks, Grounded theory coding, Health disparities, Response process, Self-rated health, Sociodemographic differences, US},
	pages = {771},
	file = {Full Text PDF:/home/pet/Zotero/storage/H3243DX4/Garbarski et al. - 2017 - How participants report their health status cognitive interviews of self-rated health across racee.pdf:application/pdf;Snapshot:/home/pet/Zotero/storage/GDSFRWDV/s12889-017-4761-2.html:text/html},
}

@inproceedings{wertz_investigating_2022,
	address = {Marseille, France},
	title = {Investigating {Active} {Learning} {Sampling} {Strategies} for {Extreme} {Multi} {Label} {Text} {Classification}},
	url = {https://aclanthology.org/2022.lrec-1.490/},
	abstract = {Large scale, multi-label text datasets with high numbers of different classes are expensive to annotate, even more so if they deal with domain specific language. In this work, we aim to build classifiers on these datasets using Active Learning in order to reduce the labeling effort. We outline the challenges when dealing with extreme multi-label settings and show the limitations of existing Active Learning strategies by focusing on their effectiveness as well as efficiency in terms of computational cost. In addition, we present five multi-label datasets which were compiled from hierarchical classification tasks to serve as benchmarks in the context of extreme multi-label classification for future experiments. Finally, we provide insight into multi-class, multi-label evaluation and present an improved classifier architecture on top of pre-trained transformer language models.},
	urldate = {2025-02-05},
	booktitle = {Proceedings of the {Thirteenth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Wertz, Lukas and Mirylenka, Katsiaryna and Kuhn, Jonas and Bogojeska, Jasmina},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Odijk, Jan and Piperidis, Stelios},
	month = jun,
	year = {2022},
	pages = {4597--4605},
}

@article{andersen_shinyrecor_2021,
	title = {{shinyReCoR}: {A} {Shiny} {Application} for {Automatically} {Coding} {Text} {Responses} {Using} {R}},
	volume = {3},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2624-8611},
	shorttitle = {{shinyReCoR}},
	url = {https://www.mdpi.com/2624-8611/3/3/30},
	doi = {10.3390/psych3030030},
	abstract = {In this paper, we introduce shinyReCoR: a new app that utilizes a cluster-based method for automatically coding open-ended text responses. Reliable coding of text responses from educational or psychological assessments requires substantial organizational and human effort. The coding of natural language in responses to tests depends on the texts’ complexity, corresponding coding guides, and the guides’ quality. Manual coding is thus not only expensive but also error-prone. With shinyReCoR, we provide a more efﬁcient alternative. The use of natural language processing makes texts utilizable for statistical methods. shinyReCoR is a Shiny app deployed as an R-package that allows users with varying technical afﬁnity to create automatic response classiﬁers through a graphical user interface based on annotated data. The present paper describes the underlying methodology, including machine learning, as well as peculiarities of the processing of language in the assessment context. The app guides users through the workﬂow with steps like text corpus compilation, semantic space building, preprocessing of the text data, and clustering. Users can adjust each step according to their needs. Finally, users are provided with an automatic response classiﬁer, which can be evaluated and tested within the process.},
	language = {en},
	number = {3},
	urldate = {2024-11-08},
	journal = {Psych},
	author = {Andersen, Nico and Zehner, Fabian},
	month = aug,
	year = {2021},
	keywords = {test},
	pages = {422--446},
	file = {PDF:/home/pet/Zotero/storage/3LJ33H7Z/Andersen und Zehner - 2021 - shinyReCoR A Shiny Application for Automatically Coding Text Responses Using R.pdf:application/pdf},
}

@inproceedings{schmidt_classifying_2023,
	title = {Classifying {Speech} {Acts} in {Political} {Communication}: {A} {Transformer}-based {Approach} with {Weak} {Supervision} and {Active} {Learning}},
	volume = {35},
	isbn = {978-83-967447-8-4},
	shorttitle = {Classifying {Speech} {Acts} in {Political} {Communication}},
	url = {https://annals-csis.org/Volume_35/drp/3485.html},
	language = {en},
	urldate = {2025-01-15},
	booktitle = {Annals of {Computer} {Science} and {Information} {Systems}},
	author = {Schmidt, Klaus and Niekler, Andreas and Kantner, Cathleen and Burghardt, Manuel},
	year = {2023},
	pages = {739--748},
}

@misc{ashvin_evaluation_2024,
	title = {Evaluation of state-of-the-art {ASR} {Models} in {Child}-{Adult} {Interactions}},
	url = {http://arxiv.org/abs/2409.16135},
	doi = {10.48550/arXiv.2409.16135},
	abstract = {The ability to reliably transcribe child-adult conversations in a clinical setting is valuable for diagnosis and understanding of numerous developmental disorders such as Autism Spectrum Disorder. Recent advances in deep learning architectures and availability of large scale transcribed data has led to development of speech foundation models that have shown dramatic improvements in ASR performance. However, the ability of these models to translate well to conversational child-adult interactions is under studied. In this work, we provide a comprehensive evaluation of ASR performance on a dataset containing child-adult interactions from autism diagnostic sessions, using Whisper, Wav2Vec2, HuBERT, and WavLM. We find that speech foundation models show a noticeable performance drop (15-20\% absolute WER) for child speech compared to adult speech in the conversational setting. Then, we employ LoRA on the best performing zero shot model (whisper-large) to probe the effectiveness of fine-tuning in a low resource setting, resulting in ∼8\% absolute WER improvement for child speech and ∼13\% absolute WER improvement for adult speech.},
	language = {en},
	urldate = {2024-12-05},
	publisher = {arXiv},
	author = {Ashvin, Aditya and Lahiri, Rimita and Kommineni, Aditya and Bishop, Somer and Lord, Catherine and Kadiri, Sudarsana Reddy and Narayanan, Shrikanth},
	month = sep,
	year = {2024},
	note = {arXiv:2409.16135 [eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	annote = {Comment: 5 pages, 3 figures, 4 tables},
	file = {PDF:/home/pet/Zotero/storage/C7JTHB6X/Ashvin et al. - 2024 - Evaluation of state-of-the-art ASR Models in Child-Adult Interactions.pdf:application/pdf},
}

@article{argyle_out_2023,
	title = {Out of {One}, {Many}: {Using} {Language} {Models} to {Simulate} {Human} {Samples}},
	volume = {31},
	shorttitle = {Out of {One}, {Many}},
	doi = {10.1017/pan.2023.2},
	abstract = {We propose and explore the possibility that language models can be studied as effective proxies for specific human subpopulations in social science research. Practical and research applications of artificial intelligence tools have sometimes been limited by problematic biases (such as racism or sexism), which are often treated as uniform properties of the models. We show that the “algorithmic bias” within one such tool—the GPT-3 language model—is instead both fine-grained and demographically correlated, meaning that proper conditioning will cause it to accurately emulate response distributions from a wide variety of human subgroups. We term this property algorithmic fidelity and explore its extent in GPT-3. We create “silicon samples” by conditioning the model on thousands of sociodemographic backstories from real human participants in multiple large surveys conducted in the United States. We then compare the silicon and human samples to demonstrate that the information contained in GPT-3 goes far beyond surface similarity. It is nuanced, multifaceted, and reflects the complex interplay between ideas, attitudes, and sociocultural context that characterize human attitudes. We suggest that language models with sufficient algorithmic fidelity thus constitute a novel and powerful tool to advance understanding of humans and society across a variety of disciplines.},
	journal = {Political Analysis},
	author = {Argyle, Lisa and Busby, Ethan and Fulda, Nancy and Gubler, Joshua and Rytting, Christopher and Wingate, David},
	month = feb,
	year = {2023},
	pages = {1--15},
	file = {Volltext:/home/pet/Zotero/storage/S9PV2JID/Argyle et al. - 2023 - Out of One, Many Using Language Models to Simulate Human Samples.pdf:application/pdf},
}

@article{baden_three_2022,
	title = {Three {Gaps} in {Computational} {Text} {Analysis} {Methods} for {Social} {Sciences}: {A} {Research} {Agenda}},
	volume = {16},
	issn = {1931-2458, 1931-2466},
	shorttitle = {Three {Gaps} in {Computational} {Text} {Analysis} {Methods} for {Social} {Sciences}},
	url = {https://www.tandfonline.com/doi/full/10.1080/19312458.2021.2015574},
	doi = {10.1080/19312458.2021.2015574},
	abstract = {We identify three gaps that limit the utility and obstruct the progress of computational text analysis methods (CTAM) for social science research. First, we contend that CTAM development has prioritized technological over validity concerns, giving limited attention to the operationalization of social scientific measurements. Second, we identify a mismatch between CTAMs’ focus on extracting specific contents and document-level patterns, and social science researchers’ need for measuring multiple, often complex contents in the text. Third, we argue that the dominance of English language tools depresses comparative research and inclusivity toward scholarly communities examining languages other than English. We substantiate our claims by drawing upon a broad review of methodological work in the computational social sciences, as well as an inventory of leading research publications using quantitative textual analysis. Subsequently, we discuss implications of these three gaps for social scientists’ uneven uptake of CTAM, as well as the field of computational social science text research as a whole. Finally, we propose a research agenda intended to bridge the identified gaps and improve the validity, utility, and inclusiveness of CTAM.},
	language = {en},
	number = {1},
	urldate = {2024-11-08},
	journal = {Communication Methods and Measures},
	author = {Baden, Christian and Pipal, Christian and Schoonvelde, Martijn and Van Der Velden, Mariken A. C. G},
	month = jan,
	year = {2022},
	pages = {1--18},
	file = {PDF:/home/pet/Zotero/storage/PAS54F32/Baden et al. - 2022 - Three Gaps in Computational Text Analysis Methods for Social Sciences A Research Agenda.PDF:application/pdf},
}

@article{bhalla_pre-testing_2023,
	title = {Pre-testing {Semi}-structured {Interview} {Questions} {Using} {Expert} {Review} and {Cognitive} {Interview} {Methods}},
	volume = {7},
	doi = {10.26666/rmp.ijbm.2023.5.2},
	abstract = {The aim of this methodological paper is to report on a way to pre-test semi-structured interview questions using both the expert review and cognitive interview methods for multiple case study research. Pre-testing, a type of pilot study, is important to ensure semi-structured interview questions can achieve the desired goal of rigour in the qualitative research process ensuring construct validity and reliability. Expert reviews can be undertaken using a modified Qualitative Appraisal System (QAS-99) questionnaire and cognitive interviews using Tourangeau's four-stage cognitive model with verbal probes and concurrent probing. Modifications can then be made to the initial semi-structured interview questions resulting in a final semi-structured interview protocol. Reflective insights in the pre-testing process should also be documented. As there is a dearth of reports on how to undertake pre-testing of semi-structured interview questions using both the expert review and cognitive interview methods, this paper provides a valuable methodological guide for qualitative researchers in the preparation and development of a semi-structured interview protocol especially for multiple case study research.},
	journal = {International Journal of Business and Management},
	author = {Bhalla, Sunil and Bahar, Nurhidayah and Kanapathy, Kanagi},
	month = oct,
	year = {2023},
	pages = {11--19},
}

@incollection{adams_conducting_2015,
	title = {Conducting {Semi}-{Structured} {Interviews}},
	isbn = {978-1-119-17138-6},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119171386.ch19},
	abstract = {Conducted conversationally with one respondent at a time, the semi-structured interview (SSI) employs a blend of closed- and open-ended questions, often accompanied by follow-up why or how questions. About one hour is considered a reasonable maximum length for SSIs in order to minimize fatigue for both interviewer and respondent. This chapter begins with a discussion on the disadvantages and advantages of SSIs. Despite the disadvantages and costs of SSIs, they offer some extraordinary benefits as well. Semi-structured interviews are superbly suited for a number of valuable tasks, particularly when more than a few of the open-ended questions require follow-up queries. The chapter presents some recommendations that can be considered when constructing an SSI guide. All in all, effectively conducted semi-structured interviews, even though labor intensive, should be worth the effort in terms of the insights and information gained.},
	language = {en},
	urldate = {2024-12-05},
	booktitle = {Handbook of {Practical} {Program} {Evaluation}},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Adams, William C.},
	year = {2015},
	doi = {10.1002/9781119171386.ch19},
	note = {Section: 19
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119171386.ch19},
	keywords = {open-ended questions, semi-structured interviews, SSI guide},
	pages = {492--505},
	file = {Full Text PDF:/home/pet/Zotero/storage/KIBKLHTF/Adams - 2015 - Conducting Semi-Structured Interviews.pdf:application/pdf;Snapshot:/home/pet/Zotero/storage/8PJNI6CQ/9781119171386.html:text/html},
}

@article{andersen_span_2023,
	title = {{\textless}span style="font-variant:small-caps;"{\textgreater}{Semi}‐automatic{\textless}/span{\textgreater} coding of {\textless}span style="font-variant:small-caps;"{\textgreater}open‐ended{\textless}/span{\textgreater} text responses in {\textless}span style="font-variant:small-caps;"{\textgreater}large‐scale{\textless}/span{\textgreater} assessments},
	volume = {39},
	issn = {0266-4909, 1365-2729},
	shorttitle = {{\textless}span style="font-variant},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/jcal.12717},
	doi = {10.1111/jcal.12717},
	abstract = {Background: In the context of large-scale educational assessments, the effort required to code open-ended text responses is considerably more expensive and time-consuming than the evaluation of multiple-choice responses because it requires trained personnel and long manual coding sessions. Aim: Our semi-supervised coding method eco (exploring coding assistant) dynamically supports human raters by automatically coding a subset of the responses.
Method: We map normalized response texts into a semantic space and cluster response vectors based on their semantic similarity. Assuming that similar codes represent semantically similar responses, we propagate codes to responses in optimally homogeneous clusters. Cluster homogeneity is assessed by strategically querying informative responses and presenting them to a human rater. Following each manual coding, the method estimates the code distribution respecting a certainty interval and assumes a homogeneous distribution if certainty exceeds a predefined threshold. If a cluster is determined to certainly comprise homogeneous responses, all remaining responses are coded accordingly automatically. We evaluated the method in a simulation using different data sets.
Results: With an average miscoding of about 3\%, the method reduced the manual coding effort by an average of about 52\%.
Conclusion: Combining the advantages of automatic and manual coding produces considerable coding accuracy and reduces the required manual effort.},
	language = {en},
	number = {3},
	urldate = {2024-11-08},
	journal = {Journal of Computer Assisted Learning},
	author = {Andersen, Nico and Zehner, Fabian and Goldhammer, Frank},
	month = jun,
	year = {2023},
	pages = {841--854},
	file = {PDF:/home/pet/Zotero/storage/6RHNCRAE/Andersen et al. - 2023 - Semi‐automatic coding of span style=font-variantsma.pdf:application/pdf},
}

@inproceedings{bonorino_smart_2024,
	title = {Smart {Surveys}: {An} {Automatic} {Survey} {Generation} and {Analysis} {Tool}},
	isbn = {978-989-758-641-5},
	shorttitle = {Smart {Surveys}},
	url = {https://www.scitepress.org/Link.aspx?doi=10.5220/0011985400003470},
	abstract = {Digital Library},
	urldate = {2024-11-15},
	author = {Bonorino, Augusto Gonzalez},
	month = nov,
	year = {2024},
	pages = {113--119},
}

@book{carmines_reliability_1979,
	title = {Reliability and {Validity} {Assessment}},
	isbn = {978-1-4522-0771-1},
	abstract = {This guide demonstrates how social scientists assess the reliability and validity of empirical measurements. This monograph is a good starting point for those who want to familiarize themselves with the current debates over "appropriate" measurement designs and strategies.},
	language = {en},
	publisher = {SAGE Publications},
	author = {Carmines, Edward G. and Zeller, Richard A.},
	month = nov,
	year = {1979},
	note = {Google-Books-ID: o5x1AwAAQBAJ},
	keywords = {Reference / Research, Social Science / Research},
}

@article{de_santis_interviewing_1980,
	title = {Interviewing as social interaction},
	volume = {2},
	issn = {1573-7837},
	url = {https://doi.org/10.1007/BF02390159},
	doi = {10.1007/BF02390159},
	abstract = {This paper, based on the premise that the interview is a unique form of social interaction and, as such, deserves greater attention, addresses the interactional dynamics of the interview by weaving experience into analysis, as well as reviewing relevant literature. Four types of interviewing situations are distinguished, with the focus of discussion on two of these types (survey-interviewing and professional-interviewing), specifically in regard to varying interactional effects. The author concentrates on the focused or indepth interview, organizing the discussion around the sequence of events which contribute to the construction of the focused interviewing situation, drawing examples from her interviewing experience. The aim is to emphasize the differences between the indepth and other types of interviewing methods.},
	language = {en},
	number = {3},
	urldate = {2024-12-05},
	journal = {Qualitative Sociology},
	author = {De Santis, Grace},
	month = jan,
	year = {1980},
	keywords = {Interactional Effect, Relevant Literature, Social Interaction, Social Issue, Social Psychology},
	pages = {72--98},
	file = {Full Text PDF:/home/pet/Zotero/storage/HIK42T43/De Santis - 1980 - Interviewing as social interaction.pdf:application/pdf},
}

@article{esuli_machines_2010,
	title = {Machines that {Learn} how to {Code} {Open}-{Ended} {Survey} {Data}},
	volume = {52},
	copyright = {https://journals.sagepub.com/page/policies/text-and-data-mining-license},
	issn = {1470-7853, 2515-2173},
	url = {https://journals.sagepub.com/doi/10.2501/S147078531020165X},
	doi = {10.2501/S147078531020165X},
	language = {en},
	number = {6},
	urldate = {2024-11-08},
	journal = {International Journal of Market Research},
	author = {Esuli, Andrea and Sebastiani, Fabrizio},
	month = nov,
	year = {2010},
	pages = {775--800},
	file = {PDF:/home/pet/Zotero/storage/J4LJZ4QM/Esuli und Sebastiani - 2010 - Machines that Learn how to Code Open-Ended Survey Data.pdf:application/pdf},
}

@article{fang_natural_2022,
	title = {Natural {Language} {Processing} for {Automated} {Classification} of {Qualitative} {Data} {From} {Interviews} of {Patients} {With} {Cancer}},
	volume = {25},
	issn = {10983015},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S109830152202040X},
	doi = {10.1016/j.jval.2022.06.004},
	abstract = {Objectives: This study sought to explore the use of novel natural language processing (NLP) methods for classifying unstructured, qualitative textual data from interviews of patients with cancer to identify patient-reported symptoms and impacts on quality of life.
Methods: We tested the ability of 4 NLP models to accurately classify text from interview transcripts as “symptom,” “quality of life impact,” and “other.” Interview data sets from patients with hepatocellular carcinoma (HCC) (n = 25), biliary tract cancer (BTC) (n = 23), and gastric cancer (n = 24) were used. Models were cross-validated with transcript subsets designated for training, validation, and testing. Multiclass classiﬁcation performance of the 4 models was evaluated at paragraph and sentence level using the HCC testing data set and analyzed by the one-versus-rest technique quantiﬁed by the receiver operating characteristic area under the curve (ROC AUC) score.
Results: NLP models accurately classiﬁed multiclass text from patient interviews. The Bidirectional Encoder Representations from Transformers model generally outperformed all other models at paragraph and sentence level. The highest predictive performance of the Bidirectional Encoder Representations from Transformers model was observed using the HCC data set to train and BTC data set to test (mean ROC AUC, 0.940 [SD 0.028]), with similarly high predictive performance using balanced and imbalanced training data sets from BTC and gastric cancer populations.
Conclusions: NLP models were accurate in predicting multiclass classiﬁcation of text from interviews of patients with cancer, with most surpassing 0.9 ROC AUC at paragraph level. NLP may be a useful tool for scaling up processing of patient interviews in clinical studies and, thus, could serve to facilitate patient input into drug development and improving patient care.},
	language = {en},
	number = {12},
	urldate = {2024-11-08},
	journal = {Value in Health},
	author = {Fang, Chao and Markuzon, Natasha and Patel, Nikunj and Rueda, Juan-David},
	month = dec,
	year = {2022},
	pages = {1995--2002},
	file = {PDF:/home/pet/Zotero/storage/F33CHM8D/Fang et al. - 2022 - Natural Language Processing for Automated Classification of Qualitative Data From Interviews of Pati.PDF:application/pdf},
}

@article{feng_towards_2024,
	title = {Towards inclusive automatic speech recognition},
	volume = {84},
	issn = {0885-2308},
	url = {https://www.sciencedirect.com/science/article/pii/S0885230823000864},
	doi = {10.1016/j.csl.2023.101567},
	abstract = {Practice and recent evidence show that state-of-the-art (SotA) automatic speech recognition (ASR) systems do not perform equally well for all speaker groups. Many factors can cause this bias against different speaker groups. This paper, for the first time, systematically quantifies and finds speech recognition bias against gender, age, regional accents and non-native accents, and investigates the origin of this bias by investigating bias cross-lingually (i.e., Dutch and Mandarin) and for two different SotA ASR architectures (a hybrid DNN-HMM and an attention based end-to-end (E2E) model) through a phoneme error analysis. The results show that only a fraction of the bias can be explained by pronunciation differences between speaker groups, and that in order to mitigate bias, language- and architecture specific solutions need to be found.},
	urldate = {2024-10-24},
	journal = {Computer Speech \& Language},
	author = {Feng, Siyuan and Halpern, Bence Mark and Kudina, Olya and Scharenborg, Odette},
	month = mar,
	year = {2024},
	keywords = {Accent, Age, Bias, Gender, Inclusive automatic speech recognition},
	pages = {101567},
	file = {PDF:/home/pet/Zotero/storage/5PSE3AGX/Feng et al. - 2024 - Towards inclusive automatic speech recognition.pdf:application/pdf;ScienceDirect Snapshot:/home/pet/Zotero/storage/XM9SL4NB/S0885230823000864.html:text/html},
}

@article{buschle_qualitative_2022,
	title = {The qualitative pretest interview for questionnaire development: outline of programme and practice},
	volume = {56},
	issn = {1573-7845},
	shorttitle = {The qualitative pretest interview for questionnaire development},
	url = {https://doi.org/10.1007/s11135-021-01156-0},
	doi = {10.1007/s11135-021-01156-0},
	abstract = {Good survey research depends on asking the right questions; it is the only way to ensure that the information collected from respondents is suitable for providing good answers to our research questions. The article discusses and advocates a comprehensive consideration of qualitative-interpretive methodology in open forms of pretesting for the evaluation of draft survey questionnaires. We outline an approach we call Qualitative Pretest Interview (QPI). It transfers the idea of negotiated common understanding in everyday communication to the clarification of meaning in draft survey questions and similar stimuli. The QPI involves ascribing interview partners the role of co-experts in this process and employing methodically integrated communication strategies. This paper focusses on how QPIs are conducted. Using an example interview, we illustrate how the particular way of qualitative pretest interviewing aims at a dialogic clarification of meaning in order to reach intersubjective understanding between participant and interviewer. In the process, we gain detailed insights into how and why a certain questionnaire might not work as intended, and ideally how this might be alleviated. QPIs pursue similar goals as Cognitive Interviews but rely more systematically on qualitative-interpretive methodology.},
	language = {en},
	number = {2},
	urldate = {2024-11-15},
	journal = {Quality \& Quantity},
	author = {Buschle, Christina and Reiter, Herwig and Bethmann, Arne},
	month = apr,
	year = {2022},
	keywords = {Cognitive interview, Measurement error, Pretest, Qualitative interview, Qualitative pretest interview, Questionnaire development},
	pages = {823--842},
	file = {Full Text PDF:/home/pet/Zotero/storage/YYWKAWUY/Buschle et al. - 2022 - The qualitative pretest interview for questionnaire development outline of programme and practice.pdf:application/pdf},
}

@article{dengel_qualitative_2023,
	title = {Qualitative {Research} {Methods} for {Large} {Language} {Models}: {Conducting} {Semi}-{Structured} {Interviews} with {ChatGPT} and {BARD} on {Computer} {Science} {Education}},
	volume = {10},
	shorttitle = {Qualitative {Research} {Methods} for {Large} {Language} {Models}},
	doi = {10.3390/informatics10040078},
	abstract = {In the current era of artificial intelligence, large language models such as ChatGPT and BARD are being increasingly used for various applications, such as language translation, text generation, and human-like conversation. The fact that these models consist of large amounts of data, including many different opinions and perspectives, could introduce the possibility of a new qualitative research approach: Due to the probabilistic character of their answers, “interviewing” these large language models could give insights into public opinions in a way that otherwise only interviews with large groups of subjects could deliver. However, it is not yet clear if qualitative content analysis research methods can be applied to interviews with these models. Evaluating the applicability of qualitative research methods to interviews with large language models could foster our understanding of their abilities and limitations. In this paper, we examine the applicability of qualitative content analysis research methods to interviews with ChatGPT in English, ChatGPT in German, and BARD in English on the relevance of computer science in K-12 education, which was used as an exemplary topic. We found that the answers produced by these models strongly depended on the provided context, and the same model could produce heavily differing results for the same questions. From these results and the insights throughout the process, we formulated guidelines for conducting and analyzing interviews with large language models. Our findings suggest that qualitative content analysis research methods can indeed be applied to interviews with large language models, but with careful consideration of contextual factors that may affect the responses produced by these models. The guidelines we provide can aid researchers and practitioners in conducting more nuanced and insightful interviews with large language models. From an overall view of our results, we generally do not recommend using interviews with large language models for research purposes, due to their highly unpredictable results. However, we suggest using these models as exploration tools for gaining different perspectives on research topics and for testing interview guidelines before conducting real-world interviews.},
	journal = {Informatics},
	author = {Dengel, Andreas and Gehrlein, Rupert and Fernes, David and Görlich, Sebastian and Maurer, Jonas and Pham, Hai and Großmann, Gabriel and Eisermann, Niklas},
	month = oct,
	year = {2023},
	pages = {78},
	file = {Volltext:/home/pet/Zotero/storage/RNL7D74T/Dengel et al. - 2023 - Qualitative Research Methods for Large Language Models Conducting Semi-Structured Interviews with C.pdf:application/pdf},
}

@book{diekmann_empirische_2022,
	address = {Reinbek bei Hamburg},
	edition = {15. Auflage, vollständig überarbeitete und erweiterte Neuausgabe August 2007, Originalausgabe},
	series = {rowohlts enzyklopädie},
	title = {Empirische {Sozialforschung}: {Grundlagen}, {Methoden}, {Anwendungen}},
	isbn = {978-3-499-55678-4},
	shorttitle = {Empirische {Sozialforschung}},
	language = {ger},
	publisher = {Rowohlt Taschenbuch Verlag},
	author = {Diekmann, Andreas},
	year = {2022},
}

@inproceedings{fischbach_comparative_2024,
	address = {Bangkok, Thailand},
	title = {A {Comparative} {Analysis} of {Speaker} {Diarization} {Models}: {Creating} a {Dataset} for {German} {Dialectal} {Speech}},
	shorttitle = {A {Comparative} {Analysis} of {Speaker} {Diarization} {Models}},
	url = {https://aclanthology.org/2024.fieldmatters-1.6},
	doi = {10.18653/v1/2024.fieldmatters-1.6},
	abstract = {Speaker diarization is a critical task in the field of computer science, aiming to assign timestamps and speaker labels to audio segments. The aim of these tests in this Publication is to find a pretrained speaker diarization pipeline capable of distinguishing dialectal speakers from each other and an explorer. To achieve this, three pipelines, namely Pyannote, CLEAVER and NeMo, are tested and compared, across various segmentation and parameterization strategies. The study considers multiple scenarios, such as the impact of threshold values for speaker recognition and overlap handling on classification accuracy. Additionally, this study aims to create a dataset for German dialect identification (DID) based on the findings from this research.},
	language = {en},
	urldate = {2024-11-04},
	booktitle = {Proceedings of the 3rd {Workshop} on {NLP} {Applications} to {Field} {Linguistics} ({Field} {Matters} 2024)},
	publisher = {Association for Computational Linguistics},
	author = {Fischbach, Lea},
	year = {2024},
	pages = {43--51},
	file = {PDF:/home/pet/Zotero/storage/3C2CA2IA/Fischbach - 2024 - A Comparative Analysis of Speaker Diarization Models Creating a Dataset for German Dialectal Speech.pdf:application/pdf},
}

@inproceedings{grandeit_using_2020,
	address = {Online},
	title = {Using {BERT} for {Qualitative} {Content} {Analysis} in {Psychosocial} {Online} {Counseling}},
	url = {https://www.aclweb.org/anthology/2020.nlpcss-1.2},
	doi = {10.18653/v1/2020.nlpcss-1.2},
	abstract = {Qualitative content analysis is a systematic method commonly used in the social sciences to analyze textual data from interviews or online discussions. However, this method usually requires high expertise and manual effort because human coders need to read, interpret, and manually annotate text passages. This is especially true if the system of categories used for annotation is complex and semantically rich. Therefore, qualitative content analysis could benefit greatly from automated coding. In this work, we investigate the usage of machine learning-based text classification models for automatic coding in the area of psychosocial online counseling. We developed a system of over 50 categories to analyze counseling conversations, labeled over 10.000 text passages manually, and evaluated the performance of different machine learning-based classifiers against human coders.},
	language = {en},
	urldate = {2024-11-08},
	booktitle = {Proceedings of the {Fourth} {Workshop} on {Natural} {Language} {Processing} and {Computational} {Social} {Science}},
	publisher = {Association for Computational Linguistics},
	author = {Grandeit, Philipp and Haberkern, Carolyn and Lang, Maximiliane and Albrecht, Jens and Lehmann, Robert},
	year = {2020},
	pages = {11--23},
	file = {PDF:/home/pet/Zotero/storage/MGULH6RY/Grandeit et al. - 2020 - Using BERT for Qualitative Content Analysis in Psychosocial Online Counseling.PDF:application/pdf},
}

@misc{hannun_deep_2014,
	title = {Deep {Speech}: {Scaling} up end-to-end speech recognition},
	shorttitle = {Deep {Speech}},
	url = {http://arxiv.org/abs/1412.5567},
	abstract = {We present a state-of-the-art speech recognition system developed using end-toend deep learning. Our architecture is signiﬁcantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a “phoneme.” Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efﬁciently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5’00, achieving 16.0\% error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.},
	language = {en},
	urldate = {2024-10-24},
	publisher = {arXiv},
	author = {Hannun, Awni and Case, Carl and Casper, Jared and Catanzaro, Bryan and Diamos, Greg and Elsen, Erich and Prenger, Ryan and Satheesh, Sanjeev and Sengupta, Shubho and Coates, Adam and Ng, Andrew Y.},
	month = dec,
	year = {2014},
	note = {arXiv:1412.5567 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {PDF:/home/pet/Zotero/storage/5BG3MLQA/Hannun et al. - 2014 - Deep Speech Scaling up end-to-end speech recognition.pdf:application/pdf},
}

@article{harris_mixing_2010,
	title = {Mixing interview and questionnaire methods: {Practical} problems in aligning data},
	volume = {15},
	language = {en},
	number = {1},
	journal = {Practical Assesment, Research \& Evaluation},
	author = {Harris, Lois R and Brown, Gavin T L},
	year = {2010},
	pages = {1--19},
	file = {PDF:/home/pet/Zotero/storage/FTEUYMQ8/Harris und Brown - Mixing interview and questionnaire methods Practical problems in aligning data.pdf:application/pdf},
}

@article{he_automatic_2020,
	title = {Automatic {Coding} of {Open}-ended {Questions} into {Multiple} {Classes}: {Whether} and {How} to {Use} {Double} {Coded} {Data}},
	shorttitle = {Automatic {Coding} of {Open}-ended {Questions} into {Multiple} {Classes}},
	url = {https://ojs.ub.uni-konstanz.de/srm/article/view/7639},
	doi = {10.18148/SRM/2020.V14I3.7639},
	abstract = {Responses to open-ended questions in surveys are usually coded into pre-specified classes, manually or automatically using a statistical learning algorithm. Automatic coding of open-ended responses relies on a set of manually coded responses, based on which a statistical learning model is fitted. In this paper, we investigate whether and how double coding can help improve the automatic classification of open-ended responses. We evaluate four strategies for training the statistical algorithm on double coded data, using experiments on simulated and real data. We find that, when the data are already double-coded (i.e. double coding does not incur additional costs), double coding where an expert resolves intercoder disagreement leads to the greatest classification accuracy. However, when we have a fixed budget for manually coding, single coding is preferable if the coding error rate is anticipated to be less than about 35\% to 45\%.},
	language = {en},
	urldate = {2024-11-08},
	journal = {Survey Research Methods},
	author = {He, Zhoushanyue and Schonlau, Matthias},
	month = aug,
	year = {2020},
	note = {Artwork Size: 267-287 Pages
Publisher: Survey Research Methods},
	pages = {267--287 Pages},
	annote = {SeriesInformation
Survey Research Methods, Vol 14 No 3 (2020)},
	file = {PDF:/home/pet/Zotero/storage/XBXXFVTC/He und Schonlau - 2020 - Automatic Coding of Open-ended Questions into Multiple Classes Whether and How to Use Double Coded.PDF:application/pdf},
}

@article{hosseini_exploratory_2023,
	title = {An exploratory survey about using {ChatGPT} in education, healthcare, and research},
	volume = {18},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0292216},
	doi = {10.1371/journal.pone.0292216},
	abstract = {Objective ChatGPT is the first large language model (LLM) to reach a large, mainstream audience. Its rapid adoption and exploration by the population at large has sparked a wide range of discussions regarding its acceptable and optimal integration in different areas. In a hybrid (virtual and in-person) panel discussion event, we examined various perspectives regarding the use of ChatGPT in education, research, and healthcare. Materials and methods We surveyed in-person and online attendees using an audience interaction platform (Slido). We quantitatively analyzed received responses on questions about the use of ChatGPT in various contexts. We compared pairwise categorical groups with a Fisher’s Exact. Furthermore, we used qualitative methods to analyze and code discussions. Results We received 420 responses from an estimated 844 participants (response rate 49.7\%). Only 40\% of the audience had tried ChatGPT. More trainees had tried ChatGPT compared with faculty. Those who had used ChatGPT were more interested in using it in a wider range of contexts going forwards. Of the three discussed contexts, the greatest uncertainty was shown about using ChatGPT in education. Pros and cons were raised during discussion for the use of this technology in education, research, and healthcare. Discussion There was a range of perspectives around the uses of ChatGPT in education, research, and healthcare, with still much uncertainty around its acceptability and optimal uses. There were different perspectives from respondents of different roles (trainee vs faculty vs staff). More discussion is needed to explore perceptions around the use of LLMs such as ChatGPT in vital sectors such as education, healthcare and research. Given involved risks and unforeseen challenges, taking a thoughtful and measured approach in adoption would reduce the likelihood of harm.},
	language = {en},
	number = {10},
	urldate = {2024-11-15},
	journal = {PLOS ONE},
	author = {Hosseini, Mohammad and Gao, Catherine A. and Liebovitz, David M. and Carvalho, Alexandre M. and Ahmad, Faraz S. and Luo, Yuan and MacDonald, Ngan and Holmes, Kristi L. and Kho, Abel},
	month = may,
	year = {2023},
	note = {Publisher: Public Library of Science},
	keywords = {Electronic medical records, Health care, Human learning, Language, Medical risk factors, Medicine and health sciences, Surveys, Trainees},
	pages = {e0292216},
	file = {Full Text PDF:/home/pet/Zotero/storage/8F7F5MJ7/Hosseini et al. - 2023 - An exploratory survey about using ChatGPT in education, healthcare, and research.pdf:application/pdf},
}

@article{he_coding_2020,
	title = {Coding {Text} {Answers} to {Open}-ended {Questions}: {Human} {Coders} and {Statistical} {Learning} {Algorithms} {Make} {Similar} {Mistakes}},
	volume = {data},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Coding {Text} {Answers} to {Open}-ended {Questions}},
	url = {https://mda.gesis.org/index.php/mda/article/view/2020.10},
	doi = {10.12758/MDA.2020.10},
	abstract = {Text answers to open-ended questions are often manually coded into one of several predefined categories or classes. More recently, researchers have begun to employ statistical models to automatically classify such text responses. It is unclear whether such automated coders and human coders find the same type of observations difficult to code or whether humans and models might be able to compensate for each other’s weaknesses. We analyze correlations between estimated error probabilities of human and automated coders and find: 1) Statistical models have higher error rates than human coders 2) Automated coders (models) and human coders tend to make similar coding mistakes. Specifically, the correlation between the estimated coding error of a statistical model and that of a human is comparable to that of two humans. 3) Two very different statistical models give highly correlated estimated coding errors. Therefore, a) the choice of statistical model does not matter, and b) having a second automated coder would be redundant.},
	language = {en},
	urldate = {2024-11-08},
	journal = {methods},
	author = {He, Zhoushanyue and Schonlau, Matthias},
	month = nov,
	year = {2020},
	note = {Artwork Size: 17 Pages
Publisher: methods, data, analyses},
	keywords = {open-ended question, manual coding, automatic coding, text classification, text answer},
	pages = {17 Pages},
	annote = {SeriesInformation
methods, data, analyses, Online First},
	annote = {SeriesInformation
methods, data, analyses, Online First},
	file = {PDF:/home/pet/Zotero/storage/V9ZMCBKV/He und Schonlau - 2020 - Coding Text Answers to Open-ended Questions Human Coders and Statistical Learning Algorithms Make S.PDF:application/pdf},
}

@article{he_automatic_2020-1,
	title = {Automatic {Coding} of {Text} {Answers} to {Open}-{Ended} {Questions}: {Should} {You} {Double} {Code} the {Training} {Data}?},
	volume = {38},
	issn = {0894-4393, 1552-8286},
	shorttitle = {Automatic {Coding} of {Text} {Answers} to {Open}-{Ended} {Questions}},
	url = {https://journals.sagepub.com/doi/10.1177/0894439319846622},
	doi = {10.1177/0894439319846622},
	abstract = {Open-ended questions in surveys are often manually coded into one of several classes (or categories). When the data are too large to manually code all texts, a statistical (or machine) learning model must be trained on a manually coded subset of texts. Uncoded texts are then coded automatically using the trained model. The quality of automatic coding depends on the trained statistical model, and the model relies on manually coded data on which it is trained. While survey scientists are acutely aware that the manual coding is not always accurate, it is not clear how double coding affects the classification errors of the statistical learning model. We investigate several budget allocation strategies when there is a limited budget for manual classification: single coding versus various options for double coding where the number of training texts is reduced to maintain the fixed budget. Under fixed budget, double coding improved prediction of the learning algorithm when the coding error is greater than about 20–35\%, depending on the data. Among double-coding strategies, paying for an expert to resolve differences performed best. When no expert is available, removing differences from the training data outperformed other double-coding strategies. When there is no budget constraint and the texts have already been double coded, all double-coding strategies generally outperformed single coding. As under fixed budget, having an expert to solve disagreement in training texts improves accuracy most, followed by removing differences.},
	language = {en},
	number = {6},
	urldate = {2024-11-08},
	journal = {Social Science Computer Review},
	author = {He, Zhoushanyue and Schonlau, Matthias},
	month = dec,
	year = {2020},
	pages = {754--765},
	file = {PDF:/home/pet/Zotero/storage/JSEV8NAB/He und Schonlau - 2020 - Automatic Coding of Text Answers to Open-Ended Questions Should You Double Code the Training Data.PDF:application/pdf},
}

@misc{huang_survey_2023,
	title = {A {Survey} of {Safety} and {Trustworthiness} of {Large} {Language} {Models} through the {Lens} of {Verification} and {Validation}},
	url = {http://arxiv.org/abs/2305.11391},
	doi = {10.48550/arXiv.2305.11391},
	abstract = {Large Language Models (LLMs) have exploded a new heatwave of AI for their ability to engage end-users in human-level conversations with detailed and articulate answers across many knowledge domains. In response to their fast adoption in many industrial applications, this survey concerns their safety and trustworthiness. First, we review known vulnerabilities and limitations of the LLMs, categorising them into inherent issues, attacks, and unintended bugs. Then, we consider if and how the Verification and Validation (V\&V) techniques, which have been widely developed for traditional software and deep learning models such as convolutional neural networks as independent processes to check the alignment of their implementations against the specifications, can be integrated and further extended throughout the lifecycle of the LLMs to provide rigorous analysis to the safety and trustworthiness of LLMs and their applications. Specifically, we consider four complementary techniques: falsification and evaluation, verification, runtime monitoring, and regulations and ethical use. In total, 370+ references are considered to support the quick understanding of the safety and trustworthiness issues from the perspective of V\&V. While intensive research has been conducted to identify the safety and trustworthiness issues, rigorous yet practical methods are called for to ensure the alignment of LLMs with safety and trustworthiness requirements.},
	urldate = {2024-11-15},
	publisher = {arXiv},
	author = {Huang, Xiaowei and Ruan, Wenjie and Huang, Wei and Jin, Gaojie and Dong, Yi and Wu, Changshun and Bensalem, Saddek and Mu, Ronghui and Qi, Yi and Zhao, Xingyu and Cai, Kaiwen and Zhang, Yanghao and Wu, Sihao and Xu, Peipei and Wu, Dengyu and Freitas, Andre and Mustafa, Mustafa A.},
	month = aug,
	year = {2023},
	note = {arXiv:2305.11391},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/home/pet/Zotero/storage/P9PVNFAN/Huang et al. - 2023 - A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and.pdf:application/pdf;Snapshot:/home/pet/Zotero/storage/9JSN23N4/2305.html:text/html},
}

@article{jansen_employing_2023,
	title = {Employing large language models in survey research},
	volume = {4},
	issn = {2949-7191},
	url = {https://www.sciencedirect.com/science/article/pii/S2949719123000171},
	doi = {10.1016/j.nlp.2023.100020},
	abstract = {This article discusses the promising potential of employing large language models (LLMs) for survey research, including generating responses to survey items. LLMs can address some of the challenges associated with survey research regarding question-wording and response bias. They can address issues relating to a lack of clarity and understanding but cannot yet correct for sampling or nonresponse bias challenges. While LLMs can assist with some of the challenges with survey research, at present, LLMs need to be used in conjunction with other methods and approaches. With thoughtful and nuanced approaches to development, LLMs can be used responsibly and beneficially while minimizing the associated risks.},
	urldate = {2024-11-15},
	journal = {Natural Language Processing Journal},
	author = {Jansen, Bernard J. and Jung, Soon-gyo and Salminen, Joni},
	month = sep,
	year = {2023},
	keywords = {Surveys, Large language models, LLM survey respondents, Survey data, Survey research},
	pages = {100020},
	file = {ScienceDirect Full Text PDF:/home/pet/Zotero/storage/83TGHUNH/Jansen et al. - 2023 - Employing large language models in survey research.pdf:application/pdf;ScienceDirect Snapshot:/home/pet/Zotero/storage/BL49RAD5/S2949719123000171.html:text/html},
}

@article{klie_inception_nodate,
	title = {The {INCEpTION} {Platform}: {Machine}-{Assisted} and {Knowledge}-{Oriented} {Interactive} {Annotation}},
	language = {en},
	author = {Klie, Jan-Christoph and Bugert, Michael and Boullosa, Beto and de Castilho, Richard Eckart and Gurevych, Iryna},
	file = {PDF:/home/pet/Zotero/storage/M5PB4Z5W/Klie et al. - The INCEpTION Platform Machine-Assisted and Knowledge-Oriented Interactive Annotation.pdf:application/pdf},
}

@article{kuhn_measuring_2023,
	title = {Measuring the {Accuracy} of {Automatic} {Speech} {Recognition} {Solutions}},
	volume = {16},
	issn = {1936-7228, 1936-7236},
	url = {https://dl.acm.org/doi/10.1145/3636513},
	doi = {10.1145/3636513},
	abstract = {For d/Deaf and hard of hearing (DHH) people, captioning is an essential accessibility tool. Significant developments in artificial intelligence mean that automatic speech recognition (ASR) is now a part of many popular applications. This makes creating captions easy and broadly available—but transcription needs high levels of accuracy to be accessible. Scientific publications and industry report very low error rates, claiming that artificial intelligence has reached human parity or even outperforms manual transcription. At the same time, the DHH community reports serious issues with the accuracy and reliability of ASR. There seems to be a mismatch between technical innovations and the real-life experience for people who depend on transcription. Independent and comprehensive data is needed to capture the state of ASR. We measured the performance of 11 common ASR services with recordings of Higher Education lectures. We evaluated the influence of technical conditions like streaming, the use of vocabularies, and differences between languages. Our results show that accuracy ranges widely between vendors and for the individual audio samples. We also measured a significant lower quality for streaming ASR, which is used for live events. Our study shows that despite the recent improvements of ASR, common services lack reliability in accuracy.},
	language = {en},
	number = {4},
	urldate = {2024-10-24},
	journal = {ACM Transactions on Accessible Computing},
	author = {Kuhn, Korbinian and Kersken, Verena and Reuter, Benedikt and Egger, Niklas and Zimmermann, Gottfried},
	month = dec,
	year = {2023},
	pages = {1--23},
	file = {PDF:/home/pet/Zotero/storage/6MSB973G/Kuhn et al. - 2023 - Measuring the Accuracy of Automatic Speech Recognition Solutions.pdf:application/pdf},
}

@inproceedings{laverghetta_jr_generating_2023,
	address = {Toronto, Canada},
	title = {Generating {Better} {Items} for {Cognitive} {Assessments} {Using} {Large} {Language} {Models}},
	url = {https://aclanthology.org/2023.bea-1.34},
	doi = {10.18653/v1/2023.bea-1.34},
	abstract = {Writing high-quality test questions (items) is critical to building educational measures but has traditionally also been a time-consuming process. One promising avenue for alleviating this is automated item generation, whereby methods from artificial intelligence (AI) are used to generate new items with minimal human intervention. Researchers have explored using large language models (LLMs) to generate new items with equivalent psychometric properties to human-written ones. But can LLMs generate items with improved psychometric properties, even when existing items have poor validity evidence? We investigate this using items from a natural language inference (NLI) dataset. We develop a novel prompting strategy based on selecting items with both the best and worst properties to use in the prompt and use GPT-3 to generate new NLI items. We find that the GPT-3 items show improved psychometric properties in many cases, whilst also possessing good content, convergent and discriminant validity evidence. Collectively, our results demonstrate the potential of employing LLMs to ease the item development process and suggest that the careful use of prompting may allow for iterative improvement of item quality.},
	urldate = {2024-11-15},
	booktitle = {Proceedings of the 18th {Workshop} on {Innovative} {Use} of {NLP} for {Building} {Educational} {Applications} ({BEA} 2023)},
	publisher = {Association for Computational Linguistics},
	author = {Laverghetta Jr., Antonio and Licato, John},
	editor = {Kochmar, Ekaterina and Burstein, Jill and Horbach, Andrea and Laarmann-Quante, Ronja and Madnani, Nitin and Tack, Anaïs and Yaneva, Victoria and Yuan, Zheng and Zesch, Torsten},
	month = jul,
	year = {2023},
	pages = {414--428},
	file = {Full Text PDF:/home/pet/Zotero/storage/7LJWR6MT/Laverghetta Jr. und Licato - 2023 - Generating Better Items for Cognitive Assessments Using Large Language Models.pdf:application/pdf},
}

@inproceedings{marathe_semi-automated_2018,
	address = {Montreal QC Canada},
	title = {Semi-{Automated} {Coding} for {Qualitative} {Research}: {A} {User}-{Centered} {Inquiry} and {Initial} {Prototypes}},
	isbn = {978-1-4503-5620-6},
	shorttitle = {Semi-{Automated} {Coding} for {Qualitative} {Research}},
	url = {https://dl.acm.org/doi/10.1145/3173574.3173922},
	doi = {10.1145/3173574.3173922},
	abstract = {Qualitative researchers perform an important and painstaking data annotation process known as coding. However, much of the process can be tedious and repetitive, becoming prohibitive for large datasets. Could coding be partially automated, and should it be? To answer this question, we interviewed researchers and observed them code interview transcripts. We found that across disciplines, researchers follow several coding practices well-suited to automation. Further, researchers desire automation after having developed a codebook and coded a subset of data, particularly in extending their coding to unseen data. Researchers also require any assistive tool to be transparent about its recommendations. Based on our ﬁndings, we built prototypes to partially automate coding using simple natural language processing techniques. Our top-performing system generates coding that matches human coders on inter-rater reliability measures. We discuss implications for interface and algorithm design, meta-issues around automating qualitative research, and suggestions for future work.},
	language = {en},
	urldate = {2024-11-08},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Marathe, Megh and Toyama, Kentaro},
	month = apr,
	year = {2018},
	pages = {1--12},
	file = {PDF:/home/pet/Zotero/storage/3HX7WKM2/Marathe und Toyama - 2018 - Semi-Automated Coding for Qualitative Research A User-Centered Inquiry and Initial Prototypes.PDF:application/pdf},
}

@article{mootz_accuracy_2024,
	title = {The {Accuracy} of {ChatGPT}-{Generated} {Responses} in {Answering} {Commonly} {Asked} {Patient} {Questions} {About} {Labor} {Epidurals}: {A} {Survey}-{Based} {Study}},
	volume = {138},
	issn = {0003-2999},
	shorttitle = {The {Accuracy} of {ChatGPT}-{Generated} {Responses} in {Answering} {Commonly} {Asked} {Patient} {Questions} {About} {Labor} {Epidurals}},
	url = {https://journals.lww.com/anesthesia-analgesia/fulltext/2024/05000/the_accuracy_of_chatgpt_generated_responses_in.27.aspx},
	doi = {10.1213/ANE.0000000000006801},
	abstract = {An abstract is unavailable.},
	language = {en-US},
	number = {5},
	urldate = {2024-11-15},
	journal = {Anesthesia \& Analgesia},
	author = {Mootz, Allison A. and Carvalho, Brendan and Sultan, Pervez and Nguyen, Teresa P. and Reale, Sharon C.},
	month = may,
	year = {2024},
	pages = {1142},
	file = {Snapshot:/home/pet/Zotero/storage/UVAY5HUQ/the_accuracy_of_chatgpt_generated_responses_in.27.html:text/html},
}

@article{poll_validity_2022,
	title = {Validity of {AI}-generated {One}-off {Questionnaires} for {Organizational} {Transformation}},
	volume = {06},
	issn = {24567760},
	url = {https://ijebmr.com/uploads/pdf/archivepdf/2022/IJEBMR_908.pdf},
	doi = {10.51505/IJEBMR.2022.6208},
	abstract = {Strategic decision-making is a precise craft that usually happens under time pressure. And often, the data for such decision-making is not (enough) available in the corporate data warehouse: you have to ask employees. This employee polling is usually very time-consuming. In this study, a novel approach is developed that couples artificial intelligence (AI) and a specific survey scale format to make one-off questionnaires in near real-time. We tested such AI-generated one-off questionnaires in 23 strategic situations where almost 7,000 employees provided nearly 6 million answers. Six statistical parameters assess the validity and reliability of the questionnaires. Our test results reveal that the developed methodology saves time and produces valid survey outcomes. Our finding, as a rule-of-thumb, is that, above a sample size of 100 respondents, AIgenerated one-off questionnaires are scoring well on the selected validity/reliability parameters. Consequently, the developed technique could be employed successfully in generating valid and reliable one-off questionnaires for organizational transformation.},
	language = {en},
	number = {02},
	urldate = {2024-11-15},
	journal = {International Journal of Economics, Business and Management Research},
	author = {Poll, Dr. Jan Van De and Yong, Yang and Weijer, Nicole Van De},
	year = {2022},
	pages = {122--130},
	file = {PDF:/home/pet/Zotero/storage/RCBZIW4Y/Poll et al. - 2022 - Validity of AI-generated One-off Questionnaires for Organizational Transformation.pdf:application/pdf},
}

@inproceedings{rietz_cody_2021,
	address = {Yokohama Japan},
	title = {Cody: {An} {AI}-{Based} {System} to {Semi}-{Automate} {Coding} for {Qualitative} {Research}},
	isbn = {978-1-4503-8096-6},
	shorttitle = {Cody},
	url = {https://dl.acm.org/doi/10.1145/3411764.3445591},
	doi = {10.1145/3411764.3445591},
	abstract = {Qualitative research can produce a rich understanding of a phenomenon but requires an essential and strenuous data annotation process known as coding. Coding can be repetitive and timeconsuming, particularly for large datasets. Existing AI-based approaches for partially automating coding, like supervised machine learning (ML) or explicit knowledge represented in code rules, require high technical literacy and lack transparency. Further, little is known about the interaction of researchers with AI-based coding assistance. We introduce Cody, an AI-based system that semiautomates coding through code rules and supervised ML. Cody supports researchers with interactively (re)defning code rules and uses ML to extend coding to unseen data. In two studies with qualitative researchers, we found that (1) code rules provide structure and transparency, (2) explanations are commonly desired but rarely used, (3) suggestions beneft coding quality rather than coding speed, increasing the intercoder reliability, calculated with Krippendorf’s Alpha, from 0.085 (MAXQDA) to 0.33 (Cody).},
	language = {en},
	urldate = {2024-11-08},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Rietz, Tim and Maedche, Alexander},
	month = may,
	year = {2021},
	pages = {1--14},
	file = {PDF:/home/pet/Zotero/storage/S7XNU38C/Rietz und Maedche - 2021 - Cody An AI-Based System to Semi-Automate Coding for Qualitative Research.pdf:application/pdf},
}

@inproceedings{rumberg_kidstalc_2022,
	title = {{kidsTALC}: {A} {Corpus} of 3- to 11-year-old {German} {Children}’s {Connected} {Natural} {Speech}},
	shorttitle = {{kidsTALC}},
	url = {https://www.isca-archive.org/interspeech_2022/rumberg22_interspeech.html},
	doi = {10.21437/Interspeech.2022-330},
	abstract = {In this paper we present kidsTALC an audio dataset with orthographic and phonetic transcriptions of German children’s speech collected to facilitate the development of speech based technological solutions. The dataset is part of a larger project aiming to develop machine-learning applications to support automation in child speech and language assessment for research and clinical purposes. At the same time, the interdisciplinary project was established to increase the accessibility of corpora of continuous child speech in Germany and globally to train accurate automated speech recognition tools for children. In the ﬁrst stage we collected and transcribed 25 hours of continuous speech from typically developing children aged 3 ½–11 years. Here, we discuss the key features of the dataset, data collection, transcription protocol and future datasets in the project. We also present important statistics of our dataset and will demonstrate the speech recognition performance of one baseline model on the dataset.},
	language = {en},
	urldate = {2024-11-08},
	booktitle = {Interspeech 2022},
	publisher = {ISCA},
	author = {Rumberg, Lars and Gebauer, Christopher and Ehlert, Hanna and Wallbaum, Maren and Bornholt, Lena and Ostermann, Jörn and Lüdtke, Ulrike},
	month = sep,
	year = {2022},
	pages = {5160--5164},
	file = {PDF:/home/pet/Zotero/storage/VKVEK3U8/Rumberg et al. - 2022 - kidsTALC A Corpus of 3- to 11-year-old German Children’s Connected Natural Speech.pdf:application/pdf},
}

@incollection{matthes_content_2017,
	edition = {1},
	title = {Content {Analysis}, {Automatic}},
	copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1.1},
	isbn = {978-1-118-90176-2 978-1-118-90173-1},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9781118901731.iecrm0043},
	abstract = {Automatic content analysis (
              ACA
              ) is a technique for coding messages with the help of computer algorithms. Unlike computer‐aided content analysis,
              ACA
              is defined as any method in which the actual coding decision, that is, assigning codes to documents or single textual or audiovisual elements, does not require human judgment and therefore is performed automatically. Since
              ACA
              relies on the computing capabilities of machines rather than human coders, it can be applied to very large samples of documents. Moreover, automatic coding is highly reliable in the sense that any analysis can be reproduced exactly given the same material and software and all errors are, in principle, deterministic, that is, stemming from misspecification or program errors.},
	language = {en},
	urldate = {2024-11-08},
	booktitle = {The {International} {Encyclopedia} of {Communication} {Research} {Methods}},
	publisher = {Wiley},
	author = {Scharkow, Michael},
	editor = {Matthes, Jörg and Davis, Christine S. and Potter, Robert F.},
	month = nov,
	year = {2017},
	doi = {10.1002/9781118901731.iecrm0043},
	pages = {1--14},
	file = {PDF:/home/pet/Zotero/storage/UFTU5T24/Scharkow - 2017 - Content Analysis, Automatic.pdf:application/pdf},
}

@article{li_qualitative_nodate,
	title = {Qualitative {Coding} in the {Computational} {Era}: {A} {Hybrid} {Approach} to {Improve} {Reliability} and {Reduce} {Effort} for {Coding} {Ethnographic} {Interviews}},
	abstract = {Sociologists have argued that there is value in incorporating computational tools into qualitative research, including using machine learning to code qualitative data. Yet standard computational approaches do not neatly align with traditional qualitative practices. The authors introduce a hybrid human-machine learning approach (HHMLA) that combines a contemporary iterative approach to qualitative coding with advanced word embedding models that allow contextual interpretation beyond what can be reliably accomplished with conventional computational approaches. The results, drawn from an analysis of 87 human-coded ethnographic interview transcripts, demonstrate that HHMLA can code data sets at a fraction of the effort of human-only strategies, saving hundreds of hours labor in even modestly sized qualitative studies, while improving coding reliability. The authors conclude that HHMLA may provide a promising model for coding data sets where human-only coding would be logistically prohibitive but conventional computational approaches would be inadequate given qualitative foci.},
	language = {en},
	author = {Li, Zhuofan and Dohan, Daniel and Abramson, Corey M},
	file = {PDF:/home/pet/Zotero/storage/Q47U7AJB/Li et al. - Qualitative Coding in the Computational Era A Hybrid Approach to Improve Reliability and Reduce Eff.PDF:application/pdf},
}

@misc{radford_robust_2022,
	title = {Robust {Speech} {Recognition} via {Large}-{Scale} {Weak} {Supervision}},
	url = {http://arxiv.org/abs/2212.04356},
	abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any ﬁnetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
	language = {en},
	urldate = {2024-10-24},
	publisher = {arXiv},
	author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
	month = dec,
	year = {2022},
	note = {arXiv:2212.04356 [eess]},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	file = {Full Text PDF:/home/pet/Zotero/storage/DN77S78C/Radford et al. - 2022 - Robust Speech Recognition via Large-Scale Weak Supervision.pdf:application/pdf;PDF:/home/pet/Zotero/storage/MPFM3F9Z/Radford et al. - 2022 - Robust Speech Recognition via Large-Scale Weak Supervision.pdf:application/pdf;Preprint PDF:/home/pet/Zotero/storage/DN9BE9TH/Radford et al. - 2022 - Robust Speech Recognition via Large-Scale Weak Supervision.pdf:application/pdf},
}

@misc{schroder_self-training_2024,
	title = {Self-{Training} for {Sample}-{Efficient} {Active} {Learning} for {Text} {Classification} with {Pre}-{Trained} {Language} {Models}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2406.09206},
	doi = {10.48550/ARXIV.2406.09206},
	abstract = {Active learning is an iterative labeling process that is used to obtain a small labeled subset, despite the absence of labeled data, thereby enabling to train a model for supervised tasks such as text classification. While active learning has made considerable progress in recent years due to improvements provided by pre-trained language models, there is untapped potential in the often neglected unlabeled portion of the data, although it is available in considerably larger quantities than the usually small set of labeled data. In this work, we investigate how self-training, a semi-supervised approach that uses a model to obtain pseudo-labels for unlabeled data, can be used to improve the efficiency of active learning for text classification. Building on a comprehensive reproduction of four previous self-training approaches, some of which are evaluated for the first time in the context of active learning or natural language processing, we introduce HAST, a new and effective self-training strategy, which is evaluated on four text classification benchmarks. Our results show that it outperforms the reproduced self-training approaches and reaches classification results comparable to previous experiments for three out of four datasets, using as little as 25\% of the data. The code is publicly available at https://github.com/chschroeder/self-training-for-sample-efficient-active-learning .},
	urldate = {2025-01-06},
	publisher = {arXiv},
	author = {Schröder, Christopher and Heyer, Gerhard},
	year = {2024},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@misc{yogarajan_tackling_2023,
	title = {Tackling {Bias} in {Pre}-trained {Language} {Models}: {Current} {Trends} and {Under}-represented {Societies}},
	shorttitle = {Tackling {Bias} in {Pre}-trained {Language} {Models}},
	url = {http://arxiv.org/abs/2312.01509},
	doi = {10.48550/arXiv.2312.01509},
	abstract = {The benefits and capabilities of pre-trained language models (LLMs) in current and future innovations are vital to any society. However, introducing and using LLMs comes with biases and discrimination, resulting in concerns about equality, diversity and fairness, and must be addressed. While understanding and acknowledging bias in LLMs and developing mitigation strategies are crucial, the generalised assumptions towards societal needs can result in disadvantages towards under-represented societies and indigenous populations. Furthermore, the ongoing changes to actual and proposed amendments to regulations and laws worldwide also impact research capabilities in tackling the bias problem. This research presents a comprehensive survey synthesising the current trends and limitations in techniques used for identifying and mitigating bias in LLMs, where the overview of methods for tackling bias are grouped into metrics, benchmark datasets, and mitigation strategies. The importance and novelty of this survey are that it explores the perspective of under-represented societies. We argue that current practices tackling the bias problem cannot simply be 'plugged in' to address the needs of under-represented societies. We use examples from New Zealand to present requirements for adopting existing techniques to under-represented societies.},
	urldate = {2024-11-15},
	publisher = {arXiv},
	author = {Yogarajan, Vithya and Dobbie, Gillian and Keegan, Te Taka and Neuwirth, Rostam J.},
	month = dec,
	year = {2023},
	note = {arXiv:2312.01509},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	file = {Preprint PDF:/home/pet/Zotero/storage/C79PLYB2/Yogarajan et al. - 2023 - Tackling Bias in Pre-trained Language Models Current Trends and Under-represented Societies.pdf:application/pdf;Snapshot:/home/pet/Zotero/storage/WTLUXRIN/2312.html:text/html},
}

@inproceedings{schroder_small-text_2023,
	address = {Dubrovnik, Croatia},
	title = {Small-{Text}: {Active} {Learning} for {Text} {Classification} in {Python}},
	shorttitle = {Small-{Text}},
	url = {https://aclanthology.org/2023.eacl-demo.11},
	doi = {10.18653/v1/2023.eacl-demo.11},
	abstract = {We introduce small-text, an easy-to-use active learning library, which offers pool-based active learning for single- and multi-label text classiﬁcation in Python. It features numerous pre-implemented state-of-the-art query strategies, including some that leverage the GPU. Standardized interfaces allow the combination of a variety of classiﬁers, query strategies, and stopping criteria, facilitating a quick mix and match, and enabling a rapid and convenient development of both active learning experiments and applications. With the objective of making various classiﬁers and query strategies accessible for active learning, small-text integrates several well-known machine learning libraries, namely scikit-learn, PyTorch, and Hugging Face transformers. The latter integrations are optionally installable extensions, so GPUs can be used but are not required. Using this new library, we investigate the performance of the recently published SetFit training paradigm, which we compare to vanilla transformer ﬁne-tuning, ﬁnding that it matches the latter in classiﬁcation accuracy while outperforming it in area under the curve. The library is available under the MIT License at https://github.com/webis-de/small-text, in version 1.3.0 at the time of writing.},
	language = {en},
	urldate = {2024-11-08},
	booktitle = {Proceedings of the 17th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Schröder, Christopher and Müller, Lydia and Niekler, Andreas and Potthast, Martin},
	year = {2023},
	pages = {84--95},
	file = {PDF:/home/pet/Zotero/storage/EB775MXB/Schröder et al. - 2023 - Small-Text Active Learning for Text Classification in Python.pdf:application/pdf},
}

@misc{suguri_motoki_charting_2024,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Charting {New} {Territory}: {Using} {ChatGPT} to {Enhance} {Survey} {Instruments} in the {Organizational} {Setting}},
	shorttitle = {Charting {New} {Territory}},
	url = {https://papers.ssrn.com/abstract=4595896},
	doi = {10.2139/ssrn.4595896},
	abstract = {This study explores how large language models (LLMs) can create synthetic survey data to make the validation of constructs and pre-test of surveys faster and more reliable. We propose a strategy and a framework for designing prompts and personas that enable LLMs to simulate human responses. We apply the framework to three validated survey instruments and collect ChatGPT-generated answers. Using this synthetic data, we conduct widely used validity tests and run the structural estimation. We find that LLMs can replicate human behavior and validate instruments developed to examine the organizational setting in a way consistent with theory, addressing challenges such as survey design, construct validity, reliability, and generalizability. Despite the limitations that we document, we posit that LLMs can help evaluate the epistemic relationships of constructs and that synthetic data can be a complement to real-world data to advance the rigor and efficiency of survey-based research.},
	language = {en},
	urldate = {2024-11-15},
	publisher = {Social Science Research Network},
	author = {Suguri Motoki, Fabio Yoshio and Monteiro, Januário and Malagueño, Ricardo and Rangel, Victor},
	month = jan,
	year = {2024},
	keywords = {Charting New Territory: Using ChatGPT to Enhance Survey Instruments in the Organizational Setting, Fabio Yoshio Suguri Motoki, Januário Monteiro, Ricardo Malagueño, SSRN, Victor Rangel},
	file = {Full Text PDF:/home/pet/Zotero/storage/SQH59DZV/Suguri Motoki et al. - 2024 - Charting New Territory Using ChatGPT to Enhance Survey Instruments in the Organizational Setting.pdf:application/pdf},
}

@misc{tjuatja_llms_2024,
	title = {Do {LLMs} exhibit human-like response biases? {A} case study in survey design},
	shorttitle = {Do {LLMs} exhibit human-like response biases?},
	url = {http://arxiv.org/abs/2311.04076},
	doi = {10.48550/arXiv.2311.04076},
	abstract = {As large language models (LLMs) become more capable, there is growing excitement about the possibility of using LLMs as proxies for humans in real-world tasks where subjective labels are desired, such as in surveys and opinion polling. One widely-cited barrier to the adoption of LLMs as proxies for humans in subjective tasks is their sensitivity to prompt wording - but interestingly, humans also display sensitivities to instruction changes in the form of response biases. We investigate the extent to which LLMs reflect human response biases, if at all. We look to survey design, where human response biases caused by changes in the wordings of "prompts" have been extensively explored in social psychology literature. Drawing from these works, we design a dataset and framework to evaluate whether LLMs exhibit human-like response biases in survey questionnaires. Our comprehensive evaluation of nine models shows that popular open and commercial LLMs generally fail to reflect human-like behavior, particularly in models that have undergone RLHF. Furthermore, even if a model shows a significant change in the same direction as humans, we find that they are sensitive to perturbations that do not elicit significant changes in humans. These results highlight the pitfalls of using LLMs as human proxies, and underscore the need for finer-grained characterizations of model behavior. Our code, dataset, and collected samples are available at https://github.com/lindiatjuatja/BiasMonkey},
	urldate = {2024-11-15},
	publisher = {arXiv},
	author = {Tjuatja, Lindia and Chen, Valerie and Wu, Sherry Tongshuang and Talwalkar, Ameet and Neubig, Graham},
	month = feb,
	year = {2024},
	note = {arXiv:2311.04076},
	keywords = {Computer Science - Computation and Language},
	annote = {Anmerkungen(21.11.2024, 13:25:17)
„In what ways do large language models (LLMs) display human-like behavior, and in what ways do they differ?“ (Tjuatja et al., 2024, p. 1)„practical implications.“ (Tjuatja et al., 2024, p. 1)„The seeming success of these models suggests that LLMs may be able to serve as viable participants in studies—such as surveys—in the same way as humans [6], allowing researchers to rapidly prototype and explore many design decisions [7, 8].“ (Tjuatja et al., 2024, p. 1)„As surveys are a primary method of choice for obtaining the subjective opinions of large-scale populations [13] and are used across a diverse set of organizations and applications [17, 18, 19], we believe that our results would be of broad interest to multiple research communities.“ (Tjuatja et al., 2024, p. 2)„response biases—have been well studied in the literature on survey design [13] and can manifest as a result of changes to the specific wording [14], format [15], and placement [16] of survey questions.“ (Tjuatja et al., 2024, p. 2)„(1) LLMs do not generally reflect human-like behaviors as a result of question modifications: All models showed behavior notably unlike humans such as a significant change in the opposite direction of known human biases, and a significant change to non-bias perturbations. Furthermore, unlike humans, models are unlikely to show significant changes due to bias modifications if they are more uncertain with their original responses.“ (Tjuatja et al., 2024, p. 2)„(2) Behavioral trends of RLHF-ed models differ from those of vanilla LLMs: RLHF-ed models demonstrated less significant changes to question modifications as a result of response biases but are more affected by non-bias perturbations, highlighting the potential undesirable effects of additional training schemes.“ (Tjuatja et al., 2024, p. 2)„(3) There is little correspondence between exhibiting response biases and other desirable metrics for survey design: We find that a model’s ability to replicate human opinion distributions is not indicative of how well an LLM reflects human behavior. These results suggest the need for care and caution when considering the use of LLMs as human proxies, as well as the importance of building more extensive evaluations that disentangle the nuances of how LLMs may or may not behave similarly to humans.“ (Tjuatja et al., 2024, p. 2)„We selected LLMs to evaluate based on multiple axes of consideration: open-source versus commercial models, whether the model has been instruction fine-tuned, whether the model has undergone reinforcement learning with human feedback (RLHF), and the number of model parameters.“ (Tjuatja et al., 2024, p. 4)„We evaluate a total of nine models, which include variants of Llama2 [29] (7b, 13b, 70b), Solar3 (an instruction fine-tuned version of Llama2 70b) and variants of the Llama2 chat family (7b, 13b, 70b), which has had both instruction fine-tuning as well as RLHF [29], along with models from the GPT series [30] (GPT 3.5 turbo, GPT 3.5 turbo instruct).4“ (Tjuatja et al., 2024, p. 4)„We conduct a comprehensive evaluation of LLMs on a set of desired behaviors that would potentially make them more suitable human proxies, using survey design as a case study.“ (Tjuatja et al., 2024, p. 9)„We also observe distinct differences in behavior between Llama2 base and their chat counterparts, which uncover the effects of additional training schemes, namely RLHF.“ (Tjuatja et al., 2024, p. 9)„In this work, the focus of our experiments was on English-based, and U.S.-centric survey questions. However, we believe that many of these evaluations can and should be replicated on corpora comprising more diverse languages and users.“ (Tjuatja et al., 2024, p. 9)„Ishita Dasgupta, Andrew K. Lampinen, Stephanie C. Y. Chan, Antonia Creswell, Dharshan Kumaran, James L. McClelland, and Felix Hill. Language models show human-like content effects on reasoning, 2022.“ (Tjuatja et al., 2024, p. 10)
},
	file = {Preprint PDF:/home/pet/Zotero/storage/J9EHUYBQ/Tjuatja et al. - 2024 - Do LLMs exhibit human-like response biases A case study in survey design.pdf:application/pdf;Snapshot:/home/pet/Zotero/storage/76389CZX/2311.html:text/html},
}

@incollection{hedderich_mit_2021,
	title = {Mit {Kindern} {Interviews} führen: {Ein} praxisorientierter Überblick},
	isbn = {978-3-7815-2454-5},
	shorttitle = {Mit {Kindern} {Interviews} führen},
	url = {https://www.pedocs.de/volltexte/2021/22252/pdf/Vogl_2021_Mit_Kindern_Interviews_fuehren.pdf},
	language = {de},
	urldate = {2024-11-27},
	booktitle = {Perspektiven auf {Vielfalt} in der frühen {Kindheit}. {Mit} {Kindern} {Diversität} erforschen},
	publisher = {Verlag Julius Klinkhardt},
	author = {Vogl, Susanne},
	editor = {Hedderich, Ingeborg and Reppin, Jeanne and Butschi, Corinne},
	month = may,
	year = {2021},
	doi = {10.35468/5895-08},
	pages = {142--157},
	file = {PDF:/home/pet/Zotero/storage/VADETVDN/Vogl - 2021 - Mit Kindern Interviews führen Ein praxisorientierter Überblick.pdf:application/pdf},
}

@misc{wuttke_ai_2024,
	title = {{AI} {Conversational} {Interviewing}: {Transforming} {Surveys} with {LLMs} as {Adaptive} {Interviewers}},
	shorttitle = {{AI} {Conversational} {Interviewing}},
	url = {http://arxiv.org/abs/2410.01824},
	abstract = {Traditional methods for eliciting people's opinions face a trade-off between depth and scale: structured surveys enable large-scale data collection but limit respondents' ability to express unanticipated thoughts in their own words, while conversational interviews provide deeper insights but are resource-intensive. This study explores the potential of replacing human interviewers with large language models (LLMs) to conduct scalable conversational interviews. Our goal is to assess the performance of AI Conversational Interviewing and to identify opportunities for improvement in a controlled environment. We conducted a small-scale, in-depth study with university students who were randomly assigned to be interviewed by either AI or human interviewers, both employing identical questionnaires on political topics. Various quantitative and qualitative measures assessed interviewer adherence to guidelines, response quality, participant engagement, and overall interview efficacy. The findings indicate the viability of AI Conversational Interviewing in producing quality data comparable to traditional methods, with the added benefit of scalability. Based on our experiences, we present specific recommendations for effective implementation.},
	urldate = {2024-11-15},
	publisher = {arXiv},
	author = {Wuttke, Alexander and Aßenmacher, Matthias and Klamm, Christopher and Lang, Max M. and Würschinger, Quirin and Kreuter, Frauke},
	month = sep,
	year = {2024},
	note = {arXiv:2410.01824 
version: 1},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {Full Text PDF:/home/pet/Zotero/storage/BRKW3ZSH/Wuttke et al. - 2024 - AI Conversational Interviewing Transforming Surveys with LLMs as Adaptive Interviewers.pdf:application/pdf;Snapshot:/home/pet/Zotero/storage/IGZFEN2L/2410.html:text/html},
}

@article{zahle_reactivity_2023,
	title = {Reactivity and good data in qualitative data collection},
	volume = {13},
	issn = {1879-4920},
	url = {https://doi.org/10.1007/s13194-023-00514-z},
	doi = {10.1007/s13194-023-00514-z},
	abstract = {Reactivity in qualitative data collection occurs when a researcher generates data about a situation with reactivity, that is, a situation in which the ongoing research affects the research participants such that they, say, diverge from their routines when the researcher is present, or tell the researcher what they think she wants to hear. In qualitative research, there are two basic approaches to reactivity. The traditional position maintains that data should ideally be collected in situations without any reactivity. In other words, good data are reactivity free. By contrast, the more recent view holds that data from situations with reactivity are fine as long as the researcher is aware of the occurring reactivity so that she can take it into account when interpreting her data. In this fashion, good data are reactivity transparent. In this paper, I first spell out and defend the more recent approach to reactivity. I argue that qualitative data are reactivity transparent when conjoined with true reactivity assumptions and that, thus supplemented, data are informative about social life independently of its being studied. Next, I examine various issues raised by the requirement to put forth true reactivity assumptions. Lastly, I use my discussion of reactivity transparency as a basis for providing a framework for thinking about good qualitative data.},
	language = {en},
	number = {1},
	urldate = {2024-12-05},
	journal = {European Journal for Philosophy of Science},
	author = {Zahle, Julie},
	month = feb,
	year = {2023},
	keywords = {Data collection, Data quality, Good data, Observer effect, Qualitative methods, Qualitative research, Reactivity},
	pages = {10},
	file = {Full Text PDF:/home/pet/Zotero/storage/V7VM725T/Zahle - 2023 - Reactivity and good data in qualitative data collection.pdf:application/pdf},
}

@article{zehner_automatic_nodate,
	title = {Automatic {Processing} of {Text} {Responses} in {Large}-{Scale} {Assessments}},
	abstract = {In automatic coding of short text responses, a computer categorizes or scores responses. In the dissertation, a free software has been developed that is capable of (i) grouping text responses into semantically homogeneous types, (ii) coding the types (e.g., correct / incorrect), and (iii) extracting further features from the responses. The software overcomes the crucial disadvantages of open-ended response formats, opens new doors for the assessment process, and makes raw responses in large-scale assessments accessible as a new source of information. Three studies analyzed n = 41,990 responses from the German sample of the Programme for International Student Assessment (PISA) 2012 with diﬀerent research interests, which were balanced according to three pillars. Publication (A) introduced the software and evaluated its performance. This involved pillar (I), which investigates, optimizes, and evaluates the algorithms and statistical models for automatic coding. Publication (B), studying how to train the software with less data, also concerned pillar (I) but then covered pillar (II) too, which deals with potential innovations in the assessment process. The article demonstrated how coding guides can be created or improved by the automatic system incorporating the variety of the empirical data. Pillar (III), attempting to add to content-related research questions, was covered in publication (C). It analyzed diﬀerences in the text responses of girls and boys to shed light on the gender gap in reading. Results showed fair to good up to excellent agreement beyond chance between the software’s and humans’ coding (76–98\%), according to publication (A). Publication (B) demonstrated that, on average, established PISA coding guides only covered about 28 percent of empirically occurring response types, and, at the same time, the software enabled the automatic expansion of the coding guides in order to cover the remaining 72 percent. Publication (C) concluded that the diﬃculties some boys face in reading were associated with a reduced availability and ﬂexibility of their cognitive situation model and their struggle to correctly identify a question’s aim. The analysis showed among others that boy-speciﬁc responses were characterized by remarkably fewer propositions, plus those few propositions turned out to be more often irrelevant than those of girl-speciﬁc responses. The ﬁndings of the studies in pillars (II) and (III) illustrate how the developed approach and software can advance research ﬁelds and innovate educational assessment. The three pillars raised by this dissertation will be fortiﬁed in further studies. One of the crucial challenges will be to balance use cases and further software development, in order to take advantage of the innovations in natural language processing while identifying the demands in the social sciences. This paper ends with a detailed discussion on limitations, implications, and directions of the presented research.},
	language = {en},
	author = {Zehner, Fabian},
	file = {PDF:/home/pet/Zotero/storage/27H724X8/Zehner - Automatic Processing of Text Responses in Large-Scale Assessments.pdf:application/pdf},
}

@misc{noauthor_httpsarxivorgpdf240916135v1_nodate,
	title = {https://arxiv.org/pdf/2409.16135v1},
	url = {https://arxiv.org/pdf/2409.16135v1},
	urldate = {2024-11-08},
	file = {https\://arxiv.org/pdf/2409.16135v1:/home/pet/Zotero/storage/ZALKYCUG/2409.pdf:application/pdf},
}

@misc{ashvin_evaluation_2024-1,
	title = {Evaluation of state-of-the-art {ASR} {Models} in {Child}-{Adult} {Interactions}},
	url = {http://arxiv.org/abs/2409.16135},
	abstract = {The ability to reliably transcribe child-adult conversations in a clinical setting is valuable for diagnosis and understanding of numerous developmental disorders such as Autism Spectrum Disorder. Recent advances in deep learning architectures and availability of large scale transcribed data has led to development of speech foundation models that have shown dramatic improvements in ASR performance. However, the ability of these models to translate well to conversational child-adult interactions is under studied. In this work, we provide a comprehensive evaluation of ASR performance on a dataset containing child-adult interactions from autism diagnostic sessions, using Whisper, Wav2Vec2, HuBERT, and WavLM. We find that speech foundation models show a noticeable performance drop (15-20\% absolute WER) for child speech compared to adult speech in the conversational setting. Then, we employ LoRA on the best performing zero shot model (whisper-large) to probe the effectiveness of fine-tuning in a low resource setting, resulting in ∼8\% absolute WER improvement for child speech and ∼13\% absolute WER improvement for adult speech.},
	language = {en},
	urldate = {2024-11-08},
	publisher = {arXiv},
	author = {Ashvin, Aditya and Lahiri, Rimita and Kommineni, Aditya and Bishop, Somer and Lord, Catherine and Kadiri, Sudarsana Reddy and Narayanan, Shrikanth},
	month = sep,
	year = {2024},
	note = {arXiv:2409.16135 [eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	annote = {Comment: 5 pages, 3 figures, 4 tables},
	file = {PDF:/home/pet/Zotero/storage/XRXC7AG6/Ashvin et al. - 2024 - Evaluation of state-of-the-art ASR Models in Child-Adult Interactions.pdf:application/pdf},
}

@incollection{baur_methoden_2014,
	address = {Wiesbaden},
	title = {Methoden der empirischen {Sozialforschung}},
	isbn = {978-3-531-18939-0},
	url = {https://doi.org/10.1007/978-3-531-18939-0_1},
	abstract = {Die Sozialwissenschaften gelten als Wirklichkeitswissenschaften, d.h. theoretische Aussagen und Prognosen müssen der Überprüfung an der Empirie statthalten. Ohne die „Methoden der empirischen Sozialforschung“ kann nicht empirisch geforscht werden, da diese die Regeln festschreiben, nach denen Daten erhoben, mit Theorien verknüpft und anschließend ausgewertet werden. Nicht umsonst sind daher die „Methoden der empirischen Sozialforschung“ unverzichtbarer Bestandteil der Ausbildung in vermutlich jedem sozialwissenschaftlichen Studiengang, sei es die Soziologie, die Politikwissenschaft oder die Erziehungswissenschaft. Und auch in anderen Studiengängen wie der Psychologie, der Anthropogeographie, der Ökonomie, den Kommunikations- und Planungswissenschaften gehört die empirische Sozialforschung zum Standardrepertoire der Disziplin.},
	language = {de},
	urldate = {2024-10-18},
	booktitle = {Handbuch {Methoden} der empirischen {Sozialforschung}},
	publisher = {Springer Fachmedien},
	author = {Baur, Nina and Blasius, Jörg},
	editor = {Baur, Nina and Blasius, Jörg},
	year = {2014},
	doi = {10.1007/978-3-531-18939-0_1},
	pages = {41--62},
	file = {Full Text PDF:/home/pet/Zotero/storage/6SFY5JJU/Baur und Blasius - 2014 - Methoden der empirischen Sozialforschung.pdf:application/pdf},
}

@article{banerjee_proposing_2024,
	title = {Proposing a {Solution} {Towards} {Avoiding} {Ecological} {Fallacy} {While} {Using} {National} {Culture} {Dimensions} in {Consumer} {Behaviour} {Studies}},
	url = {https://jim.imibh.edu.in/pages/table-of-contents/fulltext/?id=59&title=Proposing+a+Solution+Towards+Avoiding+Ecological+Fallacy+While+Using+National+Culture+Dimensions+in+Consumer+Behaviour+Studies},
	doi = {10.1177/ijim.231200085},
	abstract = {Today web-enabled services ranging from information sharing to online purchasing of products or services have become an increasingly frequent affair. In this context, previous research has explored the nature of complex cyber transactions and the various factors, which lead to formation of perceptions about such web-based services in the consumers’ minds. One important factor, which has been intriguing management researchers in this respect, is the influence of cultural values of consumers on online purchase intentions and perceptions about e-service quality. Most studies done in this context have used Hofstede’s national-level culture dimensions to do an individual-level analysis of consumer behaviour, but in doing so, have encountered a methodological issue known as ecological fallacy. This study wishes to demonstrate an alternate approach for culture-specific consumer purchase behaviour research using individual-level cultural values as study variables, thus eradicating the issue of ecological fallacy.},
	language = {en},
	urldate = {2024-12-16},
	journal = {IMIB Journal of Innovation and Management},
	author = {Banerjee, Pratyush},
	month = jan,
	year = {2024},
	file = {PDF:/home/pet/Zotero/storage/7HSATZZU/Banerjee - 2024 - Proposing a Solution Towards Avoiding Ecological Fallacy While Using National Culture Dimensions in.pdf:application/pdf},
}

@inproceedings{connolly_orientation_2012,
	title = {{ORIENTATION} {AS} {A} {SOCIALIZATION} {PROCESS}},
	url = {https://www.semanticscholar.org/paper/ORIENTATION-AS-A-SOCIALIZATION-PROCESS-Connolly/da01dd6109c2d407aceadd38a00c9e3e69b91be4#citing-papers},
	abstract = {ED 031 226 Jc 690 266 By, Comolly, John J. Viewing Faculty Orientation as a Sociahzation Process. Pub Date (69] Note-20p. EDRS Price MF-\$0.25 1-10-\$1.10 Descriptors -*Faculty, *Jvnior Colleges, *Orientation, *Socialization Faculty orientation usually provides only information on the college's programs; even this modest goal is rarely met. The author looks at Orientation as a process of socializationacquiring attitudes, values, skills, and -appropriate social behavior. Besides department and course obiectives, college and instructional goals, student characteristics, administrative procedures, etc., the new faculty member wants to know the norms assigned to his role and expected by his group in this subsystem of higher education. A faculty recruited from so many backgrounds tests the college's ability to correct deficiencies or to delete or change values contrary to its image. To do this, the college must make its norms known, provide both rationale and motive for any change, and confine its concern to students, college, and community. Values are ranked as: primarythose the college must impress on new members; secondary, not shared by all the staff, perhaps even contentious; tertiary, possibly antrthetical, certainty without general support. Factors shaping these values are; group interaction, a ranking member of the system, contimal reinforcement and support of the value, encouragement rather than mandate, exposure of both sides of the issue, arousal and safisfaction of a need, points of agreement rather than difference, and credibility of information source (whether peer -group or superior). Essential to success of the process, of course, is cooperation of incumbent faculty. (HH)},
	urldate = {2024-12-16},
	author = {Connolly, J.},
	year = {2012},
	file = {Full Text PDF:/home/pet/Zotero/storage/9ZWDPX7U/Connolly - 2012 - ORIENTATION AS A SOCIALIZATION PROCESS.pdf:application/pdf},
}

@article{do_augmented_2024,
	title = {The {Augmented} {Social} {Scientist}: {Using} {Sequential} {Transfer} {Learning} to {Annotate} {Millions} of {Texts} with {Human}-{Level} {Accuracy}},
	volume = {53},
	issn = {0049-1241},
	shorttitle = {The {Augmented} {Social} {Scientist}},
	url = {https://doi.org/10.1177/00491241221134526},
	doi = {10.1177/00491241221134526},
	abstract = {The last decade witnessed a spectacular rise in the volume of available textual data. With this new abundance came the question of how to analyze it. In the social sciences, scholars mostly resorted to two well-established approaches, human annotation on sampled data on the one hand (either performed by the researcher, or outsourced to microworkers), and quantitative methods on the other. Each approach has its own merits - a potentially very fine-grained analysis for the former, a very scalable one for the latter - but the combination of these two properties has not yielded highly accurate results so far. Leveraging recent advances in sequential transfer learning, we demonstrate via an experiment that an expert can train a precise, efficient automatic classifier in a very limited amount of time. We also show that, under certain conditions, expert-trained models produce better annotations than humans themselves. We demonstrate these points using a classic research question in the sociology of journalism, the rise of a “horse race” coverage of politics. We conclude that recent advances in transfer learning help us augment ourselves when analyzing unstructured data.},
	language = {en},
	number = {3},
	urldate = {2024-10-18},
	journal = {Sociological Methods \& Research},
	author = {Do, Salomé and Ollion, Étienne and Shen, Rubing},
	month = aug,
	year = {2024},
	note = {Publisher: SAGE Publications Inc},
	pages = {1167--1200},
	file = {SAGE PDF Full Text:/home/pet/Zotero/storage/G66LKNMV/Do et al. - 2024 - The Augmented Social Scientist Using Sequential Transfer Learning to Annotate Millions of Texts wit.pdf:application/pdf},
}

@article{gladys_bichanga_analysis_2024,
	title = {Analysis on {How} {Worldviews} and {Value} {Systems} are {Shaped} by {Culture} and {Identity}},
	copyright = {Creative Commons Attribution 4.0 International},
	issn = {2349-7831},
	url = {https://zenodo.org/doi/10.5281/zenodo.10997891},
	doi = {10.5281/ZENODO.10997891},
	abstract = {Purpose of the paper: One of the social challenges in the modern business environment that is connected to value systems and worldviews is global leadership. The phrase "worldview" refers to a collection of moral principles, values, and beliefs that shape a variety of social facets, such as how to resolve conflicts in society and how people see their place in it. This paper begins with an introduction, describing worldviews and values, and then goes on to discuss a theoretical relationship between the two. We end with the claim that understanding coalitional and oppositional efforts carried out by social groups in daily life can be facilitated by studying worldviews.},
	language = {en},
	urldate = {2024-12-16},
	author = {Gladys Bichanga and Dr. Edward Katue Nzinga (PhD)},
	month = apr,
	year = {2024},
	note = {Publisher: Paper Publications},
	keywords = {global leadership, modern business environment, social challenges, systems and worldviews},
	file = {PDF:/home/pet/Zotero/storage/VU2SKF24/Gladys Bichanga und Dr. Edward Katue Nzinga (PhD) - 2024 - Analysis on How Worldviews and Value Systems are Shaped by Culture and Identity.pdf:application/pdf},
}

@article{koltko-rivera_psychology_2004,
	title = {The {Psychology} of {Worldviews}},
	volume = {8},
	copyright = {https://journals.sagepub.com/page/policies/text-and-data-mining-license},
	issn = {1089-2680, 1939-1552},
	url = {https://journals.sagepub.com/doi/10.1037/1089-2680.8.1.3},
	doi = {10.1037/1089-2680.8.1.3},
	abstract = {A worldview (or “world view”) is a set of assumptions about physical and social reality that may have powerful effects on cognition and behavior. Lacking a comprehensive model or formal theory up to now, the construct has been underused. This article advances theory by addressing these gaps. Worldview is defined. Major approaches to worldview are critically reviewed. Lines of evidence are described regarding worldview as a justifiable construct in psychology. Worldviews are distinguished from schemas. A collated model of a worldview's component dimensions is described. An integrated theory of worldview function is outlined, relating worldview to personality traits, motivation, affect, cognition, behavior, and culture. A worldview research agenda is outlined for personality and social psychology (including positive and peace psychology).},
	language = {en},
	number = {1},
	urldate = {2024-12-16},
	journal = {Review of General Psychology},
	author = {Koltko-Rivera, Mark E.},
	month = mar,
	year = {2004},
	pages = {3--58},
}

@article{koltko-rivera_psychology_2004-1,
	title = {The {Psychology} of {Worldviews}},
	volume = {8},
	copyright = {https://journals.sagepub.com/page/policies/text-and-data-mining-license},
	issn = {1089-2680, 1939-1552},
	url = {https://journals.sagepub.com/doi/10.1037/1089-2680.8.1.3},
	doi = {10.1037/1089-2680.8.1.3},
	abstract = {A worldview (or “world view”) is a set of assumptions about physical and social reality that may have powerful effects on cognition and behavior. Lacking a comprehensive model or formal theory up to now, the construct has been underused. This article advances theory by addressing these gaps. Worldview is defined. Major approaches to worldview are critically reviewed. Lines of evidence are described regarding worldview as a justifiable construct in psychology. Worldviews are distinguished from schemas. A collated model of a worldview's component dimensions is described. An integrated theory of worldview function is outlined, relating worldview to personality traits, motivation, affect, cognition, behavior, and culture. A worldview research agenda is outlined for personality and social psychology (including positive and peace psychology).},
	language = {en},
	number = {1},
	urldate = {2024-12-16},
	journal = {Review of General Psychology},
	author = {Koltko-Rivera, Mark E.},
	month = mar,
	year = {2004},
	pages = {3--58},
}

@article{kroh_how_2023,
	title = {How do {Children} {Rate} {Their} {Health}? {An} {Investigation} of {Considered} {Health} {Dimensions}, {Health} {Factors}, and {Assessment} {Strategies}},
	volume = {16},
	issn = {1874-897X, 1874-8988},
	shorttitle = {How do {Children} {Rate} {Their} {Health}?},
	url = {https://link.springer.com/10.1007/s12187-023-10066-6},
	doi = {10.1007/s12187-023-10066-6},
	abstract = {In large-scale surveys of both children and adults, self-rated health (SRH) based on questions such as “In general, how would you rate your health?” is a widely used measurement to assess individuals’ health status. However, while a large number of studies have investigated the health aspects people consider for their responses, and some studies show deeper insights into the assessment strategies in answering this question for adults, it is largely unknown how children assess their health based on those questions. Therefore, this study examines how children rate their health according to this question in a sample of 54 9- to 12-year-olds. By using techniques of cognitive interviewing and qualitative and quantitative content analysis, we investigate the health dimensions, health factors as well as diferent assessment strategies that children refer to in their self-assessment of general health. Our results indicate that children in this age group mostly refer to their physical health and daily functioning or consider health more non-specifcally. They also show that children take into account a wide range of specifc health aspects, with some minor diferences between subgroups, especially by gender. Additionally, our study highlights that children use several assessment strategies. Finally, our results indicate that the majority of children assess their health only using one health dimension, but a substantial share of children refect on several health factors and combine diferent assessment strategies. We conclude that children refer to comparable health dimensions and health factors, but use somewhat diferent assessment strategies compared with studies focusing on adults.},
	language = {en},
	number = {6},
	urldate = {2024-11-07},
	journal = {Child Indicators Research},
	author = {Kroh, Jacqueline and Tuppat, Julia and Gentile, Raffaela and Reichelt, Hanna},
	month = dec,
	year = {2023},
	pages = {2545--2580},
	annote = {Self Rated Health: SHR

„However, while a large number of studies have investigated the health aspects people consider for their responses, and some studies show deeper insights into the assessment strategies in answering this question for adults, it is largely unknown how children assess their health based on those questions“ (Kroh et al., 2023, p. 2545)

„It is argued that while responding to the question, individuals evaluate diferent dimensions of their health referring to, for instance, physical health, mental health, and daily functioning“ (Kroh et al., 2023, p. 2546)


Question asked first --{\textgreater} then probing questions in order to understand what children were thinking about
(Kroh et al., 2023, p. 2555)
(Kroh et al., 2023, p. 2560)
„While the children in our study named health factors such as temporary illnesses, chronic diseases, health-related behaviors, pain and symptoms, injuries, individual afect, or nonspecifc illnesses, adults were shown to consider further aspects, such as health care utilization, aging processes, or external factors (e.g., family background, role as a parent, or available health-related resources) (see Garbarski et al., 2017). In particular, health care utilization seems to be an important factor for adults’ ratings, but it is not the case for the children in our sample. Regarding the assessment strategies, we also observe some diferences compared to cognitive interviews with adults:“ (Kroh et al., 2023, p. 2573)
„s and adults’ cognitive processes have two important implications for research. They challenge the validity of comparisons of SRH across age groups as well as over the life course. Thus, we suggest that future research should explore this matter in much more detail.“ (Kroh et al., 2023, p. 2574)
},
	file = {PDF:/home/pet/Zotero/storage/LS2HKT6J/Kroh et al. - 2023 - How do Children Rate Their Health An Investigation of Considered Health Dimensions, Health Factors,.pdf:application/pdf},
}

@inproceedings{leonhardt_unlocking_2023,
	address = {Singapore},
	title = {Unlocking the {Heterogeneous} {Landscape} of {Big} {Data} {NLP} with {DUUI}},
	url = {https://aclanthology.org/2023.findings-emnlp.29},
	doi = {10.18653/v1/2023.findings-emnlp.29},
	abstract = {Automatic analysis of large corpora is a complex task, especially in terms of time efficiency. This complexity is increased by the fact that flexible, extensible text analysis requires the continuous integration of ever new tools. Since there are no adequate frameworks for these purposes in the field of NLP, and especially in the context of UIMA, that are not outdated or unusable for security reasons, we present a new approach to address the latter task: Docker Unified UIMA Interface (DUUI), a scalable, flexible, lightweight, and feature-rich framework for automatic distributed analysis of text corpora that leverages Big Data experience and virtualization with Docker. We evaluate DUUI's communication approach against a state-of-the-art approach and demonstrate its outstanding behavior in terms of time efficiency, enabling the analysis of big text data.},
	urldate = {2024-10-24},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Leonhardt, Alexander and Abrami, Giuseppe and Baumartz, Daniel and Mehler, Alexander},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {385--399},
	annote = {Docker Unified UIMA (Unstructured Information Management) Interface


Framework für automatisierte Analyse großer Textkorpora in NLP



Hauptmerkmale: 


Zeitoptimierung


Optimierte Verarbeitung großer Datensätze


Bessere Performance




Tool-Integration


Modulare Architektur (Docker-Container)


Integration verschiedener Analysis Engines und NLP-Tools


Unterstützt versch. Sprachen und Bibliotheken




Reproduzierbarkeit


Alle Annotationen, Modelle und Einstellungen können reproduziert werden


Wichtige Funktion für Validierung




Monitoring \& Logging:


Dynamische Überwachung und Fehlerberichterstattung


Transparenz und Rückverfolgbarkeit des Verarbeitungsstatus




},
	annote = {„DUUI is available on GitHub1“ (Leonhardt et al., 2023, p. 386)
„Reproducible \& reusable annotations: Evaluating automatically generated annotations requires tracking the engines or pipelines that created them. This is important, e.g., if the data is to be shared among scientists. Thus, any application of an NLP pipeline should be reproducible, just as these pi“ (Leonhardt et al., 2023, p. 386)
},
	file = {Full Text PDF:/home/pet/Zotero/storage/D9N6P276/Leonhardt et al. - 2023 - Unlocking the Heterogeneous Landscape of Big Data NLP with DUUI.pdf:application/pdf},
}

@article{robken_methoden_nodate,
	title = {Methoden empirischer {Sozialforschung}},
	language = {de},
	author = {Röbken, Dr Heinke},
	file = {PDF:/home/pet/Zotero/storage/6XY38Z48/Röbken - Methoden empirischer Sozialforschung.pdf:application/pdf},
}

@article{trommsdorff_future_1983,
	title = {{FUTURE} {ORIENTATION} {AND} {SOCIALIZATION}*},
	volume = {18},
	issn = {0020-7594, 1464-066X},
	url = {https://onlinelibrary.wiley.com/doi/10.1080/00207598308247489},
	doi = {10.1080/00207598308247489},
	abstract = {The present paper deals with the question of what kind of relationship may exist between future orientation and socialization. First, the nature of future orientation and related person variables are discussed. It is shown that a functional theory of personality and social behavior is needed in order to specify the relations between these variables. On the basis of such a theory the next question on the development of future orientation may be dealt with more fruitfully. Besides cognitive maturation, social experiences determine which kind of future orientation develops and can be adapted in different social situations. Finally, the question is dealt with to what extent future orientation (not only of parents and teachers, but also of the person to be socialized) may influence the process and result of socialization. Some preliminary data indicate the usefulness of an interactionist theory for the study on the relation between socialization and future orientation.
          , 
            La présente étude traite de la question de savoir quelles sont les relations existants entre l'orientation future et la socialisation. Tout d'abord, il y a lieu de discuter de la nature de l'orientation future et, par conséquent, d'étudier les variables de personnes qui y sont afférentes. Il est prouvé qu'une théorie fonctionnelle de la personnalité et du comportement social est nécessaire pour spécifier les relations entre ces variables. C'est sur base d'une telle théorie qu'on peut traiter d'une façon plus fructueuse de la question de l'évolution de l'orientation future. Parallèlement au processus de maturité cognitive, les expériences sociales déterminent le sens dans lequel l'orientation future évoluera et agira dans des situations sociales diverses. Enfin sera traitée la question de savoir dans quelle mesure l'orientation future (donnée non seulement par les professeurs et les parents, mais aussi par les personnes socialisées) influe sur le processus de socialisation et sur son résultat. Quelques résultats provisoires signalent l'utilité d'une théorie de l'interaction pour l'étude des relations entre l'orientation future et la socialisation.},
	language = {en},
	number = {1-4},
	urldate = {2024-12-16},
	journal = {International Journal of Psychology},
	author = {Trommsdorff, Gisela},
	month = feb,
	year = {1983},
	pages = {381--406},
	file = {PDF:/home/pet/Zotero/storage/LKUDXD9Y/Trommsdorff - 1983 - FUTURE ORIENTATION AND SOCIALIZATION.pdf:application/pdf},
}

@misc{noauthor_mixing_nodate,
	title = {Mixing interview and questionnaire methods: {Practical} problems in aligning data},
	shorttitle = {Mixing interview and questionnaire methods},
	url = {https://www.researchgate.net/publication/233871179_Mixing_interview_and_questionnaire_methods_Practical_problems_in_aligning_data},
	abstract = {Access 135+ million publications and connect with 20+ million researchers. Join for free and gain visibility by uploading your research.},
	language = {en},
	urldate = {2024-12-05},
	journal = {ResearchGate},
	file = {Snapshot:/home/pet/Zotero/storage/9MFSU7E4/233871179_Mixing_interview_and_questionnaire_methods_Practical_problems_in_aligning_data.html:text/html},
}

@misc{noauthor_empirische_nodate,
	title = {Empirische {Sozialforschung}: {Grundlagen}, {Methoden}, {Anwendungen} : {Diekmann}, {Andreas}: {Amazon}.de: {Books}},
	url = {https://www.amazon.de/Empirische-Sozialforschung-Grundlagen-Methoden-Anwendungen/dp/3499556782},
	urldate = {2024-12-05},
	file = {Empirische Sozialforschung\: Grundlagen, Methoden, Anwendungen \: Diekmann, Andreas\: Amazon.de\: Books:/home/pet/Zotero/storage/CN4YXX5Q/3499556782.html:text/html},
}

@inproceedings{roux_qualitative_2022,
	title = {Qualitative {Evaluation} of {Language} {Model} {Rescoring} in {Automatic} {Speech} {Recognition}},
	url = {https://www.isca-archive.org/interspeech_2022/roux22_interspeech.html},
	doi = {10.21437/Interspeech.2022-10931},
	language = {en},
	urldate = {2025-05-07},
	booktitle = {Interspeech 2022},
	publisher = {ISCA},
	author = {Roux, Thibault Bañeras and Rouvier, Mickael and Wottawa, Jane and Dufour, Richard},
	month = sep,
	year = {2022},
	pages = {3968--3972},
}
